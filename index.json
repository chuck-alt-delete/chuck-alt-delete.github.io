[{"content":"Introduction This lab is modified from one of my courses over at Confluent Education. The course is about using Confluent’s Ansible playbooks, but I had so much fun setting up the lab environment that I thought others would too. This lab environment is cool because you can simulate your own Virtual Private Cloud locally using Ansible’s Docker connection. I hope you have fun!\n   Learning Objectives This lab has two purposes:\n  Have fun with Ansible’s Docker connection to sumulate a Virtual Private Cloud, locally\n  Practice some Ansible fundamentals\n   Here are the Ansible fundamental’s we’ll be looking at:\n       Inventories\n  Playbooks\n     Variables\n  Ansible Vault\n     Modules\n  Roles\n        Lecture Ansible Architecture    Ansible provides an automation engine to configure remote systems at scale. Ansible uses YAML (Yet Another Markup Language) files to provide a straightforward syntax rather than endless bash scripts. Ansible strives to make its configurations idempotent, which means that it can be run multiple times with the same effect as running once. This helps to save time on subsequent runs by skipping tasks that don’t need to be repeated.\n Ansible has a \u0026#34;push\u0026#34; architecture. There is a central machine called the Ansible control node (or controller) that has ssh access to a set of target hosts. There are no extra servers, daemons, or databases required.\n Ansible is written in Python, and comes with many built-in Python programs called modules. The Ansible control node establishes an ssh connection to the target hosts, loads the modules onto the hosts, runs them, and then removes them after it is finished.\n There is a vibrant Ansible community that provides much more functionality through the Ansible Galaxy website, including custom modules, roles, and collections.\n    Activity The purpose of this activity is to practice hands-on with a typical Ansible workflow, which includes:\n   Installing Ansible in a Python virtual environment\n  Inspecting a host inventory file with multiple groups\n  Using a module to execute code remotely on an Ansible target host\n  Inspecting playbooks\n  Defining variables under the group_vars directory\n  Encrypting variables with Ansible Vault\n   Prerequisites   Python 3\n  Docker\n  Docker Compose\n  Clone the source repository\n    Install Ansible Ansible is a tool written in Python. It is considered a best practice in Python to install dependencies in a \u0026#34;virtual environment\u0026#34; to avoid conflicts with the system’s Python packages. Here, we install Ansible in a Python virtual environment.\n  Create a virtual environment called testenv located in the root of this repo.\npython3 -m venv ./testenv       The -m option specifies a module to run, which in this case is the venv module.       Activate the virtual environment.\nsource ./testenv/bin/activate       The prompt should now show (testenv) to indicate that you are using the virtual environment. We can now use python and pip instead of python3 and pip3, since the virtual environment was created with Python 3. You can verify this with python --version.       Update pip, the Python package manager\npython -m pip install --upgrade pip     Install Ansible with pip\npython -m pip install ansible-core     Install some required \u0026#34;collections\u0026#34; with ansible-galaxy, Ansible’s own \u0026#34;package manager\u0026#34; of sorts.\nansible-galaxy install -r requirements.yml \u0026amp;\u0026amp; \\ ansible-galaxy collection list       Collections are public Ansible repositories available on Ansible Galaxy. In this case, the collections have already been installed for you.         Explore a Host Inventory This lab environment uses Docker Compose to simulate virtual machines running in a remote datacenter.\n  Inspect docker-compose.yml and create the hosts host1.test.confluent and host2.test.confluent.\ndocker-compose up -d       The -d option means \u0026#34;detached\u0026#34;, which means that server logs won’t print to standard output.       Inspect the host inventory file hosts.yml. What do you notice? What do you wonder? Write down a few thoughts and compare to the sample response.\n sample response Ansible uses an inventory file to describe the hosts it will configure. The creator of the inventory file can categorize different hosts into groups and label the groups whatever they want.\n The only group Ansible requires is all, which refers to all hosts defined anywhere in the inventory. The variables (vars) defined under the all group apply to all hosts. This is where ansible_connection, and other global variables are defined. In this case, we use the docker connection to connect to docker hosts, but usually this connection will be ssh.\n In Ansible, become refers to which user is used on the target host. Usually become is set to true and the user is some user with root privileges. This allows the Ansible control node to use elevated privileges to install software.\n This inventory file has two user-defined groups of hosts:\n   atlanta — this group is called \u0026#34;atlanta\u0026#34;, perhaps to specify that hosts in this group are geographically located in Atlanta\n  boston — again, this group is probably named to denote the geographical region of the hosts\n         Run a Module on the Hosts Ansible uses modules to run programs on remote hosts. There are many modules built into Ansible, and more modules can be installed through Ansible Galaxy collections.\n  Run the ping module against the hosts in the atlanta group.\nansible -m ping \\ -i hosts.yml \\ atlanta       You should see output from the host that responds with \u0026#34;pong\u0026#34;       Run the ping module against the hosts in the boston group.\nansible -m ping \\ -i hosts.yml \\ boston     Run the ping module against all hosts.\nansible -m ping \\ -i hosts.yml \\ all       Explore Playbooks In the Ansible world, a playbook is a YAML file that defines what will execute on target hosts.\n  Inspect the file playbook_atlanta.yml. What do you notice? What do you wonder? Write down a few thoughts and compare against the sample response.\n sample response This playbook defines a couple of tasks to be run againsts hosts in the atlanta group. A task specifies a human-readable name, a module, some specifications for the module, and perhaps some task-specific variables.\n There is a variable called awesome_string that is set to the value of another variable, called vault_awesome_string using Jinja variable templating with double brackets — \u0026#34;{{ …​ }}\u0026#34;. The actual value of the vault_awesome_string variable will be explored in the next section.\n The first task uses the built-in shell module to run a shell command and register the output to a new variable called response.\n The second task uses the debug module to output some contents of the response variable.\n The third task uses the built-in yum module to install the Apache httpd webserver package with the yum package manager.\n     Run the playbook_atlanta.yml playbook against the host inventory\nansible-playbook \\ playbook_atlanta.yml \\ -i hosts.yml       Notice the tasks only ran on hosts in the atlanta group, as specified in the playbook.       Inspect the file playbook_all.yml. What do you notice? What do you wonder? Write down a few thoughts and compare against the sample response.\n sample response   This playbook runs against all hosts instead of just the atlanta hosts\n  The playbook uses inventory_hostname and group_names variables, which are built-in Ansible variables that capture information about each host\n  This playbook imports the other playbook, so playbook_atlanta.yml will run against the hosts in the atlanta group as well\n       Run the playbook_all.yml playbook against the host inventory and notice what happens.\nansible-playbook \\ playbook_all.yml \\ -i hosts.yml       Set Group Variables with group_vars and Encrypt Secrets with Ansible Vault It is common practice to use a group_vars directory to define variables for different groups of hosts. Furthermore, it is important to encrypt variables with sensitive credentials so they aren’t compromised in source control.\n  Notice that there is a directory called group_vars at the same level as the inventory file hosts.yml in `. Further notice that under `group_vars, there is another directory called atlanta. This is no accident. Ansible looks for directories under group_vars whose names correspond to host groups. Any variables defined in YAML files in group_vars/atlanta will be available for hosts in that group.\n  Inspect the file group_vars/atlanta/vault.yml. Notice that this is where the variable vault_awesome_string is defined for hosts in the atlanta group.\n  It is vital to encrypt sensitive credentials so they aren’t exposed in source control. Use Ansible Vault to encrypt vault.yml using the password confluent when prompted.\nansible-vault encrypt group_vars/atlanta/vault.yml       You should now see the contents of vault.yml have been encrypted. If you want to view the unencrypted contents, you can run ansible-vault view group_vars/atlanta/vault.yml.       Run playbook_atlanta.yml on the hosts again, using vault password confluent.\nansible-playbook --ask-vault-pass \\ playbook_atlanta.yml \\ -i hosts.yml       Notice the tasks are able to access the encrypted variables. Note that it is not secure to use the debug module to view encrypted secrets in the task output. This was only done for demonstration purposes. If you want to suppress the output of certain tasks, you can set the built-in no_log variable to True on those tasks.         (Optional) Create a Sample Role Playbooks can get crowded and hard to reason about. Ansible uses the concept of a role to package up related tasks to be shared and referenced in playbooks.\n  Create an Ansible role called sample-role.\nansible-galaxy role init sample-role     Take note of the folder structure and inspect some of the files. This will be discussed more in the activity debrief.\n    Clean Up  Destroy your hosts.\ndocker-compose down         Activity Debrief What is an Ansible Role?  tasks The tasks directory is the most important part of the role. The files in this directory define the tasks that will run on the hosts. When first learning what a role does, it is a good idea to start in this directory.\n    templates Templates generate files that will end up on the hosts. Ansible uses the Jinja templating engine, which enables files to be created with conditional logic and variables. Here is an example of a Jinja template from Ansible Playbooks for Confluent Platform that generates Kafka broker server.properties files:\n server.properties.j2 1 2 3 4 5 # Maintained by CP Ansible {# kafka_broker_final_properties defined in the confluent.variables role #} {% for key, value in kafka_broker_final_properties|dictsort%} {{key}}={{value}} {% endfor %}    Line 1 is text that will appear literally in the server.properties file.\n Line 2 is a Jinja comment, so it won’t appear in the server.properties file.\n Line 3 shows a for loop that iterates through a dictionary and sorts the dictionary with Jinja’s built-in dictsort filter.\n Line 4 puts text in the file according to the values of those variables.\n Line 5 ends the for loop.\n The end result is for several lines of text to appear in the file according the entries in the kafka_broker_final_properties dictionary.\n It is important to note how Jinja uses braces {} to set an execution context and double braces {{}} to reference variables.\n    defaults The values given to variables in this defaults directory are default values that are used if you don’t override them elsewhere. These values have the lowest precendence and are easily overridden. Usually, you assign values to variables in the appropriate group_vars subdirectory alongside your host inventory file, and any variables you didn’t explicitly assign will be given their default values from this defaults directory.\n There is a reference to variable precedence documentation in the References section.\n    handlers The handlers directory is home to special tasks called handlers. A handler is a task that triggers only when the state of something changes. A handler is notified of a change using the notify keyword.\n Here is an example of a task in tasks/main.yml that notifies a handler called restart kafka.\n tasks/main.yml - name: Write Service Overrides template: src: override.conf.j2 dest: \u0026#34;{{kafka_broker.systemd_override}}\u0026#34; mode: 0640 owner: \u0026#34;{{kafka_broker_user}}\u0026#34; group: \u0026#34;{{kafka_broker_group}}\u0026#34; notify: restart kafka   And here is the corresponding handler in the handler directory:\n handlers/main.yml - name: restart kafka systemd: daemon_reload: true name: \u0026#34;{{kafka_broker_service_name}}\u0026#34; state: restarted   In this example, the task creates a systemd service override file from a template. If the file doesn’t exist or changes, it notifies the handler named restart kafka. The restart kafka handler uses the systemd module to reload systemd and restart the Kafka service.\n    meta Files in the meta directory provide information about the role itself (metadata). This could include information about the author of the role, the namespace on Ansible Galaxy where you can find the role, role dependencies, and other metadata.\n    tests This directory usually has a sample inventory file, e.g. test-hosts.yml, that points to localhost or hosts in a test environment. The inventory would be alongside a playbook, e.g. test.yml, that calls the role with import_role. Runing the playbook tests the role on the test hosts.\n In practice, many role authors choose to use Ansible Molecule as a testing framework. Ansible Molecule uses Docker to create hosts and test different scenarios. We will see this in a later learning module.\n    Playbooks can get crowded and hard to reason about. Ansible uses the concept of a role to package up related tasks to be shared and referenced in playbooks with an import_role task. Roles are organized into all the parts shown in this slide.\n This is just a brief overview. In later learning modules, you will look at roles in the CP Ansible project in more detail.\n  Discussion Questions  What do the different parts of this Ansible command do?\nansible -m ping \\ -i hosts.yml \\ boston    sample response This command executes the ping module on hosts in the boston group of the hosts.yml inventory file.\n     What do the different parts of this Ansible command do?\nansible-playbook \\ playbook_atlanta.yml \\ -i hosts.yml    sample response This command runs the playbook playbook_atlanta.yml file on hosts in the hosts.yml file.\n     What is the group_vars directory and how does it work to define variables for different groups of hosts?\n sample response Ansible looks for directories under group_vars whose names correspond to host groups defined in a host inventory file.\n Example: Any variables defined in YAML files in the group_vars/all/ directory will be available for all hosts.\n Example: If there is a host group called atlanta, then the variables defined in YAML files in the group_vars/atlanta/ directory will be available for hosts in the atlanta group.\n     What is Ansible Vault and why is it important?\n sample response Ansible Vault refers to the ansible-vault command line utility. Ansible Vault allows you to encrypt files that contain sensitive credentails. This is important because Ansible projects are often source controlled in shared code repositories, and the set of people who have read access to the repository may be different from the set of people who should have access to the sensitive credential.\n The password for a file encrypted with Ansible Vault should be stored securely in a password manager or secure credential storage service like Hashicorp Vault. A common workflow is to authorize the Ansible control node to access relevant Ansible Vault passwords from a credential storage service and then retrieve the passwords at runtime using a password client script.\n         Summary In this module, you practiced hands-on with a typical Ansible workflow:\n   Installing Ansible in a Python virtual environment\n  Inspecting a host inventory file with multiple groups\n  Using a module to execute code remotely on an Ansible target host\n  Inspecting playbooks\n  Defining variables under the group_vars directory\n  Encrypting variables with Ansible Vault\n   With this workflow, you reviewed fundamental Ansible concepts:\n       Inventories\n  Playbooks\n     Variables\n  Ansible Vault\n     Modules\n  Roles\n      References   Ansible documentation\n  YAML Syntax Reference\n  Index of all Ansible modules\n  Ansible Vault documentation\n  Ansible Variable Precedence\n     Ansible Galaxy\n  Comprehensive Ansible 101 video series by Jeff Geerling\n  Ansible for DevOps Examples by Jeff Geerling\n  List of built-in Jinja filters\n      ","permalink":"https://chuck-alt-delete.github.io/posts/ansible-docker/","summary":"Introduction This lab is modified from one of my courses over at Confluent Education. The course is about using Confluent’s Ansible playbooks, but I had so much fun setting up the lab environment that I thought others would too. This lab environment is cool because you can simulate your own Virtual Private Cloud locally using Ansible’s Docker connection. I hope you have fun!\n   Learning Objectives This lab has two purposes:","title":"Fun with Ansible's Docker Connection"},{"content":"   Foreword From 2021 A while ago, I listened to an amazing podcast episode about DynamoDB decided to do one of AWS’s DynamoDB hands-on tutorials and create a little presentation about what I learned to share at a meetup.\n Enjoy!\n    Objectives   Get to know each other\n  Compare and contrast DynamoDB data modeling to relational data modeling\n  Describe the importance of partitioning in DynamoDB\n      Non-Work Facts About Me!     Chuck\n     My first job was at a doggy daycare\n  I randomly know a lot about aphids\n  I graduated with high honors in pure math and produced a thesis as an undergraduate\n  I love building PCs to play big beautiful video games\n  I am a podcast nerd:\n  Freakonomics, Software Engineering Daily, Intelligence Squared, Not Another DnD Podcast, Radiolab, and more\n     My wife Caroline, my son Rafael, and my son Arlo are my best friends and make me laugh every day\n         Notice and Wonder        What do you notice?\n What do you wonder?\n       Notice and Wonder Debrief      NoSQL vs. Relational Data Modeling — Online Gaming  \n Think of an online multiplayer game.\n     What are some \u0026#34;entities\u0026#34; involved?\n What are some access patterns?\n     \n    The Relational Way Suppose we want to query for all the usernames of users in a particular game.\n     Star Schema Pros Cons       Table for \u0026#34;user\u0026#34;\n  Table for \u0026#34;game\u0026#34;\n  Fact table that relates \u0026#34;user_ID\u0026#34; with \u0026#34;game_ID\u0026#34;\n  Join \u0026#34;user\u0026#34; and \u0026#34;game\u0026#34; tables and filter for records with the given \u0026#34;game_ID\u0026#34;\n     Great for ad-hoc queries\n  Great for analytics, like aggregations\n     Joins don’t scale well\n         The DynamoDB Way   This is all one big, hyper-scalable table! It looks like two entity tables and a join table, but really it’s one big table where the entities and join are created by clever design of the primary and sort keys, which we’ll see in the next slide.\n   Note  One table with O(1) lookup by primary key, or O(log(n)) if including a sort key. No joins! You plan the access pattern into your data model and include everything in one table.        Design the Primary Key Table 1. Composite Primary Key     Entity Partition Key (a.k.a. HASH) Sort Key (a.k.a. RANGE)     User\n USER#\u0026lt;USERNAME\u0026gt;\n #PLACEHOLDER#\u0026lt;USERNAME\u0026gt;\n   Game\n GAME#\u0026lt;GAME_ID\u0026gt;\n #PLACEHOLDER#\u0026lt;GAME_ID\u0026gt;\n   UserGameMapping\n GAME#\u0026lt;GAME_ID\u0026gt;\n USER#\u0026lt;USERNAME\u0026gt;\n       Code Example Query multiple entities in one request to one table!\n Take a moment to think about what this query does:\n game_id = input(\u0026#34;what game ID do you want to look up?\u0026#34;) resp = dynamodb.query( TableName=\u0026#39;battle-royale\u0026#39;, KeyConditionExpression=\u0026#34;PK = :pk AND SK BETWEEN :placeholder AND :users\u0026#34;, ExpressionAttributeValues={ \u0026#34;:pk\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;GAME#{}\u0026#34;.format(game_id) }, \u0026#34;:placeholder\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;#PLACEHOLDER#{}\u0026#34;.format(game_id) }, \u0026#34;:users\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;USER$\u0026#34; }, }, ScanIndexForward=True )   sample response This query takes a given game ID and looks up all the users in that game. Let’s say the game ID is abc123. The PK part of the KeyConditionExpression stands for \u0026#34;Primary Key\u0026#34;, which is the key used to determine which DynamoDB partition holds the data.\n Now that we have found the partition where game ID abc123 lives, the next step is to use the sort key, SK, to grab all the users. We look up records in order from :placeholder to :users, which are further specified in the ExpressionAttributeValues parameter. The :placeholder expression corresponds to the string #PLACEHOLDER#abc123 (type denoted by S). The expression :users corresponds to the string USER$.\n The # symbol is 35 in ascii, and $ is 36, so USER$ is greater than all USER#\u0026lt;USERNAME\u0026gt; entries. That means this query will return the game followed by all the users in that game in ascending order by username. For example:\n {\u0026#34;GAME#abc123\u0026#34; : \u0026#34;#PLACEHOLDER#abc123\u0026#34;} {\u0026#34;GAME#abc123\u0026#34;: \u0026#34;USER#myuser1\u0026#34;} {\u0026#34;GAME#abc123\u0026#34;: \u0026#34;USER#myuser2\u0026#34;} {\u0026#34;GAME#abc123\u0026#34;: \u0026#34;USER#myuser3\u0026#34;} ...       Next Step: Secondary Indexes Secondary indexes are very useful in DynamoDB. They allow you to query data by attributes other than the primary key. This opens up more access patterns. The benefits of secondary indexes come at the cost ($ literally $) of more reads and writes as DynamoDB automatically updates indexes. It is easy to use secondary indexes poorly, so I encourage you to dig into the tutorial in the reference to learn more about best practices with secondary indexes. Namely:\n   Sparse secondary index\n  Create an index that filters down to a specific subset that won’t grow indefinitely\n  example: create index to find all open games on a specific map\n     Inverted index\n  Switch the roles of primary key and sort key\n  example: find all games a user has played (opposite of what we did earlier when we found all users in a specific game)\n         3, 2, 1 Reflection   What are 3 things you learned?\n  What are 2 things you found interesting?\n  What’s 1 question you still have?\n      Objectives   Get to know each other\n  Describe the importance of partitioning in DynamoDB\n  Compare and contrast DynamoDB data modeling to relational data modeling\n      References   DynamoDB Hands-on Tutorials\n  Software Engineering Daily Podcast: DynamoDB with Alex Debrie\n  DynamoDB Python Reference (boto3 library)\n  AWS Docs: Improving Data Access with Secondary Indexes\n     ","permalink":"https://chuck-alt-delete.github.io/posts/dynamo-presentation/","summary":"Foreword From 2021 A while ago, I listened to an amazing podcast episode about DynamoDB decided to do one of AWS’s DynamoDB hands-on tutorials and create a little presentation about what I learned to share at a meetup.\n Enjoy!\n    Objectives   Get to know each other\n  Compare and contrast DynamoDB data modeling to relational data modeling\n  Describe the importance of partitioning in DynamoDB","title":"Learning DynamoDB"},{"content":"Foreword from 2021 I wrote this post for Insight Data Science in 2019. I\u0026rsquo;ve updated it a tiny bit, but it\u0026rsquo;s mostly the same. It was published on Dzone and the Insight Data Science blog:\n https://dzone.com/articles/how-to-prepare-for-your-devops-interview (40k views!) https://blog.insightdatascience.com/devops-interview-prep-guide-8cd3dc60e587  Introduction Motivation is a great start, but you\u0026rsquo;ll need the discipline to prepare effectively. I\u0026rsquo;ve written this guide to help you develop a disciplined interview preparation routine. Interviewing can really suck - it\u0026rsquo;s random and can make you feel bad about yourself. There\u0026rsquo;s so much you can\u0026rsquo;t control in the process, but you can control how you prepare. There is a lot here to chew on, so don\u0026rsquo;t expect to be able to learn everything all at once. Maybe one day you\u0026rsquo;ll choose one topic to focus on. Maybe you\u0026rsquo;ll focus on another topic for a week straight. In general, though, you should pick two or three activities to do habitually each day, rotating through different topics so your mind stays fresh and engaged. These categories are listed in order of importance:\n Linux Fundamentals Data Structures and Algorithms System Design Parsing DevOps Tools  It\u0026rsquo;s good to develop a wide, shallow base of knowledge first, so load balance across topics in a round-robin fashion at first. That will widen your conceptual framework of the DevOps space so that you can more easily put new information into context. The activities within each category are ranked more or less in order of importance as well. Certain activities lend themselves to daily practice, like CodeSignal problems or OverTheWire challenges. Others lend themselves to deep study.\nIf you want to go fast, go alone. But if you want to go far, go together. Create a study group with your peers. Hold each other accountable. Do weekly or daily check-ins and info-sharing sessions. During those time that you feel unmotivated, remember that doing something is much better than doing nothing. During those times when you are too motivated, remember that sleep and exercise are every bit as important to your success as anything else. These activities are the stepping stones to the next phase of your life, but remember to allow yourself to become fascinated with the shape of each stone.\nLinux Fundamentals As a DevOps Engineer, you\u0026rsquo;re expected to know Linux as well as the back of your hand. Any questions about Linux is fair game, from the kernel to networking to command line commands.\nActivities:  Work through The Linux Command Line book Work through the OverTheWire wargames. These are fun scavenger hunts through Linux systems that are well-designed to teach you the fundamental concepts. At least get through the Bandit levels. Don\u0026rsquo;t be afraid to Google solutions if you get stuck. I highly suggest you document your progress in GitHub as you go. Learn vim by simply using the vimtutor command. It\u0026rsquo;s just the best editor ever and don\u0026rsquo;t let anyone convince you otherwise. Consider paying for acloudguru courses to get certifications in Linux fundamentals (Linux+, RedHat, etc.). It\u0026rsquo;s true that a pound of certifications is less valuable than an ounce of experience, but when you are low on experience, certifications can help fill in gaps in your knowledge and prove that you have a basic understanding.  References  TLDR.sh: The best reference manual for Linux commands. I highly suggest you download the tl;dr mobile app for quick reference. This has helped me on phone interviews where I didn\u0026rsquo;t know a command off the top of my head. devhints.io is a great source for all manner of cheat sheets. It doesn\u0026rsquo;t look flashy, but the Advanced Bash Scripting Guide is a goldmine.  Data Structures and Algorithms Remember that DevOps is Development + Operations, so you\u0026rsquo;re expected to know how to code. Choose a programming language and practice, practice, practice. It\u0026rsquo;s not just about finding an optimal solution - you must clearly communicate the parameters of the problem with your interviewer, convey your thought process as you go along, and calculate the time and space complexity of different algorithms.\nActivities  Khan Academy has a really nice data structures and algorithms primer. It\u0026rsquo;s in Javascript, but don\u0026rsquo;t hold that against it. This curriculum was made in partnership with Tom Cormen, the author of the definitive book on data structures and algorithms that\u0026rsquo;s definitely worth owning a copy of. LeetCode has a large repository of problems that are close to real interview questions. In fact, many companies will directly ask you LeetCode questions! Do at least 10 easy and medium questions of each type of algorithms. You can start to feel prepared when you\u0026rsquo;ve done 100 problems and can actually solve each without looking anything up. Work through the interview prep material daily on CodeSignal. I suggest Python, but you can work in any language. Copy and paste your code into files and version control it with GitHub. Each day, look through the code you did yesterday and document it in detail. Writing code is easier than reading your old code and trying to remember what\u0026rsquo;s going on. Revisiting your old code after a day or two will force you to synthesize the concepts you are learning. Work through CodeSignal\u0026rsquo;s Python Arcade. It is a nice tour of all the important standard Python libraries. Some of the exercises really miss the mark but, overall, this is a nice playlist. Research Dynamic Programming, Directed Acyclic Graphs, Topological Sort. Everyone needs a little SQL. No need to spend large amounts of time on this, but you should sign up for a free account on Mode Analytics and make use of their great interactive SQL tutorials that use real public databases. As you get more comfortable with your coding skills, use Pramp to practice live technical interviews. This is an important component. Nothing quite prepares you for technical phone screens like practicing technical phone screens. Remember that the only knowledge that counts is what you can demonstrate in front of the interviewer, either on the whiteboard or live coding. If you tend to freeze up in high-pressure situations, you\u0026rsquo;re far from the only one - be sure to do lots of practice with peers.  References  LearnXinYminutes is a great, example-driven cheat sheet for Python Hitchhiker\u0026#39;s Guide to Python: The best Python documentation. My scripts GitHub repo has lots of examples of thoroughly documented code, although it\u0026rsquo;s really unorganized!  System Design The system design interview tests your big-picture thinking, ability to architect complex infrastructure, and communication skills. Be sure to practice a top-down design approach with clear infrastructure diagrams and DevOps considerations.\nActivities  Chip away at the System Design Primer. One fun thing to do is to pick a topic and ask a friend to pick another topic, then spend 2 hours researching your topic before coming back together for an hour or two to share. Take turns playing the roles of interviewer and interviewee. Read the Google SRE Book. This is a must-read book for modern DevOps.  Parsing Sysadmins frequently analyze system outputs and logs to obtain system insights and debug issues. Command-line parsing with tools like grep, awk, and sed is an important skill for the DevOps arsenal.\nActivities  Learn regular expressions (RegEx) with hands-on exercises at RegExOne Work through the exercises in the Insight parsing workshop materials. Focus on awk. The chapter materials are good, but the exercises are the most important part to do.  References  LearnXinYminutes: Another great example-driven reference - this time for awk. Regexr: The best regular expression reference.  DevOps Tools Docker, Terraform, and Kubernetes are essential DevOps tools, and you can\u0026rsquo;t go wrong learning them thoroughly. You should also know at least one CI/CD tool, such as Jenkins. It\u0026rsquo;s highly recommended that you learn these tools on at least one of the big three clouds: AWS, GCP, or Azure.\nActivities Containers You need to learn Kubernetes. In order to learn Kubernetes, you need to learn about containerization and Docker.\n Work through the Docker for Beginners lab Work through the Play With Docker Classroom  Kubernetes (concept: container orchestration)  Read through Kubernetes concepts Work through the Kubernetes Basics tutorial Check out Jeff Geerling\u0026rsquo;s awesome Kubernetes 101 video series For a deeper dive, work through the Katacoda course  Continuous Integration / Continuous Delivery  Create some automated testing and/or deployment on one of your GitHub repos with GitHub Actions Work through an introductory Jenkins tutorial or two to learn more about CI/CD (concept: deployment pipeline as code). I honestly don\u0026rsquo;t know that much about Jenkins, so I don\u0026rsquo;t know better resources for this.  Infrastructure and Configuration as Code  Use Vagrant to declaratively define a VM, and then use VirtualBox or Packer to export that VM as an OVA file or other machine image (concepts: immutable infrastructure, configuration as code). Terraform (concept: infrastructure as code)  Read Terragrunt\u0026rsquo;s Comprehensive Guide to Terraform Work through the Terraform Tutorial   Ansible (concept: configuration as code)  Check out Jeff Geerling\u0026rsquo;s awesome Ansible 101 video series For some hands-on labs, I think Linux Academy might be the best bet (not free)    Observability, Secrets Management, Service Mesh, Big Data  Do the Honeycomb Quickstart to give a flavor of monitoring/tracing tools (concept: full stack tracing) Work through the Vault tutorial (concept: enterprise-scale secrets management) Work through the Consul tutorial (concept: service mesh). Play with Linkerd (concept: service mesh) Apache Kafka (concept: fault tolerant, high throughput event streaming)  Shameless plug for Apache Kafka - go to developer.confluent.io to get started, and if you like that, ask your employer to pay for the education subscription (that\u0026rsquo;s what I write!)    References  Ansible: The User Guide Want to do something specific in Kubernetes? Check the Tasks section of the docs. There are many task-specific guides. Docker\u0026rsquo;s Samples page has good examples of fully built applications as well as some good hands-on tutorials. I like the Django\u0026#43;Postgres sample. Awesome Docker Compose is a great repository of docker-compose examples  Other High Value Items Activities  Listen regularly to the Software Engineering Daily podcast. This is by far the best way to keep a finger on the pulse of the DevOps world. I suggest getting the app so that you can easily search through old episodes for in-depth talks about the technologies and concepts you are studying. In the DevOps Tools section, I gave keywords for the concepts behind each tool. You can use these keywords to search SEdaily for relevant podcasts. Read about Kanban boards and use them to enhance your focus and productivity. GitHub has a simple kanban board you can use to keep yourself productive. I also personally like bullet journaling and the Pomodoro Method. Dig deep into the technical details of some of your favorite tools. Start with the documentation. Pair up and share your takeaways from the technical deep-dives. Consider paying for Linux Academy or acloudguru courses to get AWS, GCP, or Azure certifications.  References Digital Ocean has great tutorials. When I Google, I tend to try\n \u0026lt;research topic\u0026gt; site:digitalocean.com  first. This is especially helpful when you are first learning to install something manually.\n","permalink":"https://chuck-alt-delete.github.io/posts/devops-interview/","summary":"Foreword from 2021 I wrote this post for Insight Data Science in 2019. I\u0026rsquo;ve updated it a tiny bit, but it\u0026rsquo;s mostly the same. It was published on Dzone and the Insight Data Science blog:\n https://dzone.com/articles/how-to-prepare-for-your-devops-interview (40k views!) https://blog.insightdatascience.com/devops-interview-prep-guide-8cd3dc60e587  Introduction Motivation is a great start, but you\u0026rsquo;ll need the discipline to prepare effectively. I\u0026rsquo;ve written this guide to help you develop a disciplined interview preparation routine. Interviewing can really suck - it\u0026rsquo;s random and can make you feel bad about yourself.","title":"How to prepare for your DevOps Interview"},{"content":"This is the first post for Chuck+Alt+Delete! Welcome!\nThis site has two purposes:\n \u0026ldquo;learn in public\u0026rdquo; through blog posts host my professional portfolio  Learn in Public I was inspired by the #LearnInPublic hashtag on twitter. Learning in public makes one feel vulnerable, but the upsides in terms of community and learning can be tremendous. Even this post might evolve over time. This is a place for rough draft thinking.\nPortfolio Hop over to my portfolio!\n","permalink":"https://chuck-alt-delete.github.io/posts/hello-world/","summary":"This is the first post for Chuck+Alt+Delete! Welcome!\nThis site has two purposes:\n \u0026ldquo;learn in public\u0026rdquo; through blog posts host my professional portfolio  Learn in Public I was inspired by the #LearnInPublic hashtag on twitter. Learning in public makes one feel vulnerable, but the upsides in terms of community and learning can be tremendous. Even this post might evolve over time. This is a place for rough draft thinking.","title":"Hello, World!"},{"content":"Most of my work these last few years has lived in private repositories, so I\u0026rsquo;ve created this site to do more \u0026ldquo;learning in public.\u0026rdquo; I will keep this page updated with exemplars of my work.\nFree Courses As a Senior Curriculum Developer at Confluent, I have written many courses related distributed systems, databases, and stream processing. Most of my courses are only available as a part of a paid subscription, but there are a couple of free samples you can check out. Go to the Confluent training website and check out:\n Automate Deployment with Confluent for Kubernetes  Whole course is free, including fully managed hands-on labs Labs are available on GitHub at https://github.com/confluentinc/confluent-kubernetes-examples In particular, I contributed heavily to:  Configure external access with host-based routing Production grade security   See my merged pull requests for details   Try It - Create an Event Streaming App with ksqlDB  First couple of modules are free    Chuckinator0 GitHub Once upon a time, I noodled around on https://github.com/chuckinator0, which I have since replaced with this \u0026ldquo;Chuck Alt Delete\u0026rdquo; personal brand. Here are a couple of highlights:\n DevOps takehome challenge scripts: A collection of programming exercises I\u0026rsquo;ve done. I might make some blog posts based on some of these. secretSanta: A simple app that assigns secret santas and emails the family. This is actual critical infrastructure for my family\u0026rsquo;s annual secret santa tradition. It ensures no one gets the same person they got last year, nor do they get their spouse.  Chuck-Confluent GitHub Most of my Confluent work is private, but you can see my commit history:\n https://github.com/chuck-confluent  ","permalink":"https://chuck-alt-delete.github.io/posts/portfolio/","summary":"Most of my work these last few years has lived in private repositories, so I\u0026rsquo;ve created this site to do more \u0026ldquo;learning in public.\u0026rdquo; I will keep this page updated with exemplars of my work.\nFree Courses As a Senior Curriculum Developer at Confluent, I have written many courses related distributed systems, databases, and stream processing. Most of my courses are only available as a part of a paid subscription, but there are a couple of free samples you can check out.","title":"Portfolio"}]