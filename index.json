[{"content":" Tip I originally wrote this for https://confluent.io/learn/api, but I wanted to host a version of it on my own blog. Introduction API stands for Application Programming Interface, which allows applications to communicate with one another. In the following image, imagine an app on the left is trying to communicate with the app on the right:\nThe key idea is the app on the left doesn’t need to know the details about how the other application works; it only needs to know about how to use the API. This is the concept of abstraction, which makes it possible for new programs to build on top of the hard work encoded into other programs.\nThere are many kinds of APIs and a mountain of technical definitions. This post will discuss only a few modern APIs in plain language with practical examples and some hands-on components.\nMetaphor: Video Games Video games make a nice metaphor for the power of abstraction that an interface provides.\nThe video game player presses buttons, and somehow the video game console knows how to take those buttons and turn them into actions in the game. The player only needs to know the buttons to press in order to have fun with the game; they do not need to know how the machine takes those inputs and uses them to render actions on the screen.\nREST API The REST API is the most popular API architecture in modern web applications. REST stands for Representational State Transfer. The underlying theory is nuanced, but in practice a REST API basically means there is a client who makes certain kinds of requests to alter resources on a server. Each request represents the state the client wants the resource to have and attempts to transfer that wish to the server. The server responds on its own without having to remember anything about the client (this is known as stateless requests).\nThe most common place you will see REST APIs at play is in your browser. You click a button, and the browser will send a request to a server to retrieve a resource and display that resource on your screen.\nREST APIs are sometimes described as”RESTful”.\nExample: cURL a Web API It’s difficult to conceptualize a REST API without a concrete example. Suppose there is a RESTful book API service hosted at https://api.fakebooksite.com. Users can create, read, update, and delete books using the API.\nThe cURL command is an application that can create HTTP requests and is commonly used to interact with REST APIs from the command line. Here is an example of a request one might submit to the books REST API server with the cURL application:\ncurl --request GET \\ \u0026#34;https://api.fakebooksite.com/v1/authors?author=chuck\u0026amp;sort=title:desc\u0026#34; Here, the curl command uses the “GET” HTTP method, so this command should retrieve a resource from the server rather than creating or updating a resource on the server. Let\u0026rsquo;s break down what the different parts of the URL (\u0026ldquo;uniform resource locator\u0026rdquo;) are doing in this API.\nurl part purpose https:// This is the part of the URL that signals what protocol will be used. In this case, we use the secure HTTP protocol. api.fakebooksite.com This part of the URL is called the domain. This is what humans use to find servers on the internet (as opposed to IP addresses). /v1/authors/ This part of the URL is the REST API endpoint. It points to resources on the server. It is often prefixed by an API version so that the API can evolve over time without upsetting those who prefer to use older versions. This endpoint points to the “authors” resource. ?author=chuck\u0026amp;sort=title:desc This part of the URL is called the query string which starts with a “?” and encodes query parameters, which are key-value pairs defined with an “=” and separated with “\u0026amp;”. In this case, we specify the name of the author and ask for the response to be sorted by title name in descending (reverse) alphabetical order. The REST API server application would be written in a way to respond to this kind of request and serve a response. In this case, the books API might retrieve information about books written by “chuck” from a database, perform a sort operation, and send a response to the cURL client in a JSON format.\nTip APIs often respond with data encoded in JSON format. The jq command is a handy tool for parsing json responses. It is useful to pipe the results of a curl command into jq to see a pretty output, like this (-X is a shortcut for --request):\ncurl -X GET https://my-cool-site.com/v1/cool-quotes | jq { \u0026#34;quotes\u0026#34;: [{ \u0026#34;author\u0026#34;: \u0026#34;Bill and Ted\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;Be excellent to each other\u0026#34; }, { \u0026#34;author\u0026#34;: \u0026#34;George Box\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;All models are wrong. Some are useful.\u0026#34; } ] } JSON stands for JavaScript Object Notation. Despite its name, it is simply a standard file format that any language can use to send and receive data over the internet. The next example will send JSON data to a REST API endpoint.\nIt’s great to receive data from a REST API endpoint, but often we also want to send a “payload” of data to a REST API. Here is an example that creates/updates the book “Chuck’s Cool Book”:\ncurl \\ --request PUT \\ --header “Content-Type: application/json” \\ --user chuck:chuck-password \\ --data ‘{ “author”: “chuck”, “title”:”Chuck’s Cool Book”, “text”: “When I was a boy, I ate 3 dozen eggs each morning…” }’ \\ \u0026#34;https://api.fakebooksite.com/v1/books\u0026#34; There are a couple of new things here to notice:\npart description --request PUT We are using a different HTTP method. “PUT” usually means we want to provide data the server can use to create a resource or replace a resource if it already exists. --header “Content-Type: application/json” Headers are key-value pairs that give the server or client additional information about the request. There are many standard headers, like “Content-Type”. In this case, the client is telling the server that it is going to send a JSON data payload. --user chuck:chuck-password cURL’s \u0026ndash;user option creates an “Authorization” header that includes username and password so the server can authenticate the client and decide if it is authorized to have its request fulfilled. In this case, it’s probably a good idea to require users to sign in before they are allowed to post books, otherwise anyone could stuff the database full of whatever books they want. It’s also important that the protocol for this API is HTTPS rather than just HTTP. HTTPS means the connection between the client and server is encrypted, so no one can eavesdrop on the connection and steal the user’s password. -data ‘{...}’ We use this option to compose our JSON payload to send to the server. In this case, we define author, title, and book text in JSON format to send to the server. In summary, the above cURL command will send an HTTP PUT request to the server at the /v1/books endpoint with a JSON payload. When the server receives the PUT request, it would authenticate the user, check whether the user is authorized to create that resource, perhaps update a database record for that book, and then return a response to the client (e.g. status code 200 to report success).\nNote The server might behave very differently depending on which HTTP method is used. In this case, a GET request to https://api.fakebooksite.com/v1/books?author=chuck\u0026amp;title=Chuck%27s\u0026#43;Cool\u0026#43;Book might not require authentication at all and give a response that includes the contents of “Chuck’s Cool Book” in JSON format. GraphQL API GraphQL is an API query language that provides an alternative to REST. GraphQL is great because it gives clients a more flexible way to query an API for exactly what they need and provides developers an easier way to evolve their APIs while maintaining backward compatibility.\nGraphQL specifies three execution operations:\nQuery (retrieve data from the server) Mutation (change data on the server) Subscription (subscribe to a stream of data events from the server) GraphQL Example: Querying Countries Go to https://lucasconstantino.github.io/graphiql-online/ to play hands-on with a graphQL API.\nHere is an example query that you can input into the GraphQL playground:\nquery { countries(filter: { code: { regex: \u0026#34;.*A.*\u0026#34; } }) { currency code name continent { name } } languages(filter: { code: { regex: \u0026#34;.*r\u0026#34; } }) { native name code } } This example illustrates how GraphQL gives the client the power to decide what data they want the server to return. With a REST API, making a GET request to a /countries endpoint would return a lot of information the client doesn’t necessarily need, and it would be up to the client to parse through the response. In addition, GraphQL empowers the client to choose data they want from a variety of resources in a single request. In this example, we also see that the client has requested information about languages. With a RESTful architecture, the client would have to make two requests, one to /countries and another to /languages.\nHere are a few things to notice about the query:\nThe query is defined with the “query” keyword within curly braces {} A filter can be applied so that only a subset of data is returned to the client E.g. for (filter:{code:{regex:\u0026quot;.*A.*\u0026quot;}}), we select only the countries whose country code contains an “A” using regular expressions. The GraphQL web editor, called GraphiQL, is sending an HTTP request to a real GraphQL endpoint. Here’s what an equivalent cURL command looks like (note the escape backslashes for double quotes and the lack of newlines in the query value):\ncurl -s -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;{ countries(filter: {code: {regex: \\\u0026#34;.*A.*\\\u0026#34;}}) { currency code name continent { name } } languages(filter: {code: {regex: \\\u0026#34;.*r\\\u0026#34;}}) { native name code }}\u0026#34;}\u0026#39; \\ https://countries.trevorblades.com | jq Next, explore the API on your own. It’s especially helpful to use Ctrl+Space to take advantage of auto-complete. Also don’t forget to explore the docs panel on the upper right. These docs are automatically generated from GraphQL’s type system (yay schemas and auto generated documentation!).\nRPC API Another popular alternative to REST API architecture is RPC, which stands for Remote Procedure Call. With an RPC API, the client sends a request to a server that asks the server to execute a “procedure” (i.e. “function”, or “method”).\nRPC is different from a REST because the endpoint in a REST API is a resource like /v1/books/, whereas the endpoint in an RPC API is an action like /v1/book.update. In other words, REST APIs are centered around nouns whereas RPC APIs are centered around verbs.\nRPC APIs are popular for synchronous communication between microservices. The most popular framework for developing RPC APIs is gRPC.\nRPC API Example: Fictional Store API This example by Arnaud Lauret is useful to illustrate the difference between REST and RPC APIs:\nOperation RPC (operation) REST (resource) Signup POST /signup POST /persons Resign POST /resign DELETE /persons/1234 Read person GET /readPerson?personid=1234 GET /persons/1234 Read person\u0026rsquo;s items list GET /readUsersItemsList?userid=1234 GET /persons/1234/items Add item to person\u0026rsquo;s list POST /addItemToUsersItemsList POST /persons/1234/items Update item POST /modifyItem PUT /items/456 Delete item POST /removeItem?itemId=456 DELETE /items/456 Source: https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/#examples\nNotice RPC APIs only use POST and GET HTTP methods, choosing to let the endpoint describe what kind of operation is taking place. Compare that to REST, where DELETE, GET, POST, and PUT methods to the same resource endpoint will result in different behaviors.\nRPC API Example: Slack API Slack is a popular enterprise collaboration app. Slack offers an RPC API to interact with its resources. Here is an example for setting a user’s profile data:\n# Set user profile data curl -X POST \\ -H “Content-Type: application/json” -H “Authorization: Bearer my-super-secret-token” \\ --data ‘{ \u0026#34;profile\u0026#34;: { \u0026#34;status_text\u0026#34;: \u0026#34;riding a train\u0026#34;, \u0026#34;status_emoji\u0026#34;: \u0026#34;:mountain_railway:\u0026#34;, \u0026#34;status_expiration\u0026#34;: 1532627506, \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john@smith.com\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;Xf06054BBB\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;Barista\u0026#34;, \u0026#34;alt\u0026#34;: \u0026#34;I make the coffee \u0026amp; the tea!\u0026#34; } } } }’ https://slack.com/api/users.profile.set?user=W1234567890 | jq The JSON data payload is taken straight from the example in Slack’s documentation. Notice the endpoint /api/users.profile.set is an action rather than a resource. Here just a few other action-oriented RPC endpoints related to users:\nusers.profile.get users.deletePhoto users.setPhoto users.setActive Slack API documentation pages for their endpoints (called “methods”, which makes sense for RPC):\nhttps://api.slack.com/methods Native Library API All the API examples so far have been APIs over the HTTP protocol (REST, GraphQL, RPC), but another common way to use APIs is directly in the code of your own application through libraries.\nA library is a collection of objects and functions that you can import into your own application to use. A library is typically tailored to solve a specific problem. Instead of solving that problem yourself, you take advantage of the library’s capabilities by using its API. This makes more sense as you dig into examples.\nLibrary API Example: Python\u0026rsquo;s JSON Library Here is a simple Python program that uses the API provided by the json library to serialize a Python dictionary into a json file.\nimport json my_dictionary = { \u0026#34;my-key\u0026#34; : [1,2,3] } with open(\u0026#34;my-file.json\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(my_dictionary, f) Here are a few things to notice:\nimport json\nThis is where we import the library. Every language provides ways to import libraries. json.dump(my_dictionary, f)\nWe can research the library’s official API documentation to find the functions and objects we need for our program. In this case, we find that the dump method does what we want — it takes a Python object and serializes it into JSON file format. The API specifies that dump has two required positional arguments — first, the object to be serialized, and second, a file writer object to write the JSON data to a file. The important point is that we can use the dump function in our program without knowing at all how it is implemented. Library API Example: Apache Kafka Producer API Apache Kafka provides APIs to read from, process, and write data to Kafka servers (called brokers). There are Kafka client libraries available in many programming languages. We’ll use Java to contrast with the previous Python example.\n… import org.apache.kafka.clients.producer.*; … public class ProducerExample { public static void main(final String[] args) throws IOException { … final Properties props = new Properties(); InputStream propsFile = new FileInputStream(\u0026#34;src/main/resources/producer.properties\u0026#34;); props.load(propsFile); KafkaProducer\u0026lt;String, String\u0026gt; producer = new KafkaProducer\u0026lt;\u0026gt;(props); String t = \u0026#34;my_topic\u0026#34; String k = \u0026#34;mykey\u0026#34;; String v = \u0026#34;myvalue\u0026#34;; ProducerRecord\u0026lt;String, String\u0026gt; record = new ProducerRecord\u0026lt;String, String\u0026gt;(t, k, v); producer.send(record); } The pattern is the same as the previous example:\nimport the library research the API documentation to learn what objects and functions are available and how to use them Profit In this case, we\ncreate a KafkaProducer object called “producer” configured with a properties file Define a topic, key, and value Create a ProducerRecord object called “record” that holds the record key, record value, and the topic where we want to send the record Best API Tools Here are some of the most popular tools for developing and using APIs:\nAwesome Docs https://developer.mozilla.org/en-US/docs/Web/HTTP \u0026ndash; Excellent reference for all things HTTP, like headers, methods, status codes, etc. https://graphql.org/learn/ https://grpc.io/docs/ Search hubs for APIs others have made https://app.swaggerhub.com/search by SmartBear https://www.postman.com/explore by Postman Tools to interact with APIs https://swagger.io \u0026ndash; Interactive online API editor demo that models a pet store REST API with Open API: https://editor.swagger.io/ https://postman.com https://insomnia.rest/ – API development tool by Kong, Inc. https://httpie.io/ \u0026ndash; httpie is a really great, modern alternative to cURL https://httpie.io/docs/cli/examples – examples that you can run interactively on https://httpie.io/cli/run Popular frameworks for developing web APIs: Python FastAPI https://fastapi.tiangolo.com/ Flask https://flask.palletsprojects.com/en/2.1.x/ Django https://www.djangoproject.com/ Java SpringBoot https://spring.io/projects/spring-boot Really helpful tool to initialize new projects: https://start.spring.io/ Quarkus https://quarkus.io/ Go Gin https://gin-gonic.com/ Gorilla http://www.gorillatoolkit.org/ GraphQL Code Generator https://www.graphql-code-generator.com/ \u0026ndash; Define your schema and generate frontend client code and backend server code OpenAPI specification https://spec.openapis.org/oas/v3.1.0 \u0026ndash; Provides a language-agnostic specification that allows you to declare an API in a JSON or YAML document file (usually called openapi.yml, swagger.yaml, or similar). Use Swagger Codegen or OpenAPI Generator to take the OpenAPI document file and generate REST API server and client code, as well as documentation, in the language or framework of your choice. AsyncAPI specification https://www.asyncapi.com/ \u0026ndash; In the same family as OpenAPI, but focused on asynchronous, event-driven APIs rather than synchronous request/response APIs. Like OpenAPI, you create an API in a YAML document file and use tools to generate code and documentation ","permalink":"https://chuck-alt-delete.github.io/posts/what-is-an-api/","summary":"Tip I originally wrote this for https://confluent.io/learn/api, but I wanted to host a version of it on my own blog. Introduction API stands for Application Programming Interface, which allows applications to communicate with one another. In the following image, imagine an app on the left is trying to communicate with the app on the right:\nThe key idea is the app on the left doesn’t need to know the details about how the other application works; it only needs to know about how to use the API.","title":"What is an API?"},{"content":"Every so often, I decide to turn my life upside down. A bit over a week ago, I made this announcement:\n📢Anouncement📢\nSurprising even myself, I have decided to move on from @confluentinc and this will be my last week. Confluent is an amazing company, and in particular, @jwfbean , David Shook, and Maygol Kananizadeh have been such amazing teammates. 1/3\n\u0026mdash; Chuck Larrieu Casias (@MrLarrieu) October 24, 2022 But why leave a great company (which, by the way, just beat expectations for the 6th consecutive quarter, despite the recession)?\nWhy leave a manager and team I love working with?\nThere are two main reasons:\nIt\u0026rsquo;s a great time for me to take risks It\u0026rsquo;s a great time for me to expand my expertise by doing something new Taking Risks I am quite confident in Confluent. I\u0026rsquo;m going to exercise and hold my vested shares. Confluent owns the data streams, ensuring its place as a key player in the real-time data space for the forseeable future. This confidence in Confluent\u0026rsquo;s future affords me the ability to take greater risk, even in current poor market conditions. I am buying another ticket in the \u0026ldquo;startup lottery\u0026rdquo;.\nI\u0026rsquo;ve had the privilege and support to take these kinds of risks several times in my life, like when I:\nDecided to become a high school math teacher (my teaching site \u0026ndash; a trip down memory lane!) Quit teaching to write math curriculum (see the free curriculum, taught to millions of students in the US) Quit writing math curriculum to teach myself DevOps (see my DevOps interview prep post) Quit writing technical curriculum to become a Technical Marketing Manager And each time was a phase change in my life for the better. Through teaching, I became more kind and empathetic. Through writing curriculum, I became a more creative and clear communicator. Through my DevOps bootcamp, I learned resilience in the face of failure and how to align my technical skill with business value. Through technical marketing, I learned to listen closely to the user and speak to their needs.\nThis new role will have much to teach me as well. To be honest, I am a little intimidated; but I am also comforted by my history of taking on challenges like this.\nDoing Something New When I write courses, I am often tempted to give one simple assignment:\nHere is a list of references. Now, write a course about this subject for someone else. Get feedback on your work from an expert and revise.\nThis is exactly how I\u0026rsquo;ve gained the technical skills I have now. I am nearly certain I learned more by creating each course than anyone who has taken that course has learned from me \u0026ndash; about Mathematics, the internals of distributed systems like Kubernetes and Apache® Kafka, programming, software architecture, data, all of it. This is because knowledge is generally not transferred from teacher to learner; rather, it is constructed by the learner (see Constructivism).\nSo far I have created labs, demos, and other technical content from the safety of the Ivory Tower. \u0026ldquo;Production\u0026rdquo; for me has meant a live workshop, conference demo, or published article. The next step in my professional journey is to build mission-critical software systems alongside real customers in production.\nIn addition to the technical challenge, I also look forward to the organizational challenge of helping to build a company from an early stage. My generalist nature and creativity lend themselves nicely to the unstructured atmosphere of an early startup. It\u0026rsquo;s a fun and interesting challenge to build on ground that shifts beneath my feet.\nMaterialize! With all of that said, today is my first day at Materialize! I will be working directly with customers as a Field Engineer under the management of excellent engineer and speaker Seth Wiesman. Seth was very convincing about the prospects of Materialize, and with my own research, I have grown very excited at the potential impact we will have on the database industry.\nMaterialize is a database. \u0026ldquo;Oh no, yet another database!\u0026rdquo; I hear you shout. Well, Materialize does something really cool, and does it really well. When you query most databases, they act like they\u0026rsquo;ve never been asked that question before. They recompute the entire answer every time you ask. This is the idea of passive data. With Materialize, you ask a query, and that query lives persistently, constantly staying up-to-date as new data arrives in real-time (a.k.a. an incrementally updated materialized view). This is the idea of active data or data in motion, and was in fact the promise of ksqlDB. In the time since ksqlDB added the \u0026ldquo;DB\u0026rdquo; part, it has for various reasons retreated towards more of a transformation tool for data pipelines and less of a database. Materialize, an amazing solution for streaming data pipelines in its own right, overcomes the limitations of ksqlDB and proudly wears the label \u0026ldquo;streaming database\u0026rdquo;.\nSome other databases also offer incrementally updated views, but the key breakthrough for Materialize is that it can maintain these views with:\nincredible performance full support of arbitrary SQL, including complex joins and aggregations strong consistency Why I\u0026rsquo;m Excited About the Tech As for the technical details, here are a couple of things that get me really excited about Materialize:\nIt\u0026rsquo;s a database built on top of a stream processing engine called Differential Dataflow, implemented in pure Rust for beastly performance. Unlike other stream processors like Flink and Kafka Streams, Differential Dataflow is designed specifically for stream processing workloads without borrowing from technologies originally created for batch or transactional workloads (see Why not RocksDB for streaming storage?). Single threaded performance is insane. I found a preliminary benchmark that shows single digit to 10,000x single threaded performance improvements over Postgres and Spark on the TCP-H standard benchmark queries The dataflows can scale horizontally as well. It guarantees strict serializability Therefore with respect to the CAP Theorem, Materialize is a CP database \u0026ndash; strongly consistent and partition tolerant. In theory, this comes at the expense of availability, but CP databases can maintain high enough availability to meet essentially any practical requirement (see my favorite blog post about this from Google Cloud Spanner) This means Materialize can support mission-critical transactional use cases that eventually consistent databases and stream processors simply can\u0026rsquo;t (e.g. financial) Why I\u0026rsquo;m Excited About the Business The competitive landscape for databases is tough, so Materialize had better have a killer go-to-market strategy. And it does! Here are a couple of reasons I\u0026rsquo;m excited about the business execution strategy:\nMaterialize is fully compatible with Postgres wire protocol, meaning teams can keep using the tools they already use (e.g. psql, language libraries, dbt, etc) \u0026ndash; frictionless adoption. Confluent CEO Jay Kreps argued that businesses are becoming software, meaning that software is taking more responsibility for making automatic business decisions. With Materialize, stream processing applications can use the same frontend -\u0026gt; web server -\u0026gt; database architecture as traditional request/response applications to realize the promise of real-time, event-driven microservices that Jay describes without having to learn or operate a particular stream processing framework or library. Materialize supports arbitrary queries from the SQL-92 standard, which means data teams don\u0026rsquo;t have to learn a new flavor of SQL and don\u0026rsquo;t even have to learn nuanced details about stream processing (e.g. Kafka topics, partitions, brokers, stream/table duality, co-partitioning, etc). Materialize integrates with Kafka, so it plugs right in to the most successful data streaming platform in the world. Materialize has first class change data capture (CDC) integration with Postgres, making it essentially the best Postgres read replica ever. It can effortlessly serve even the most complex joins and aggregations, always up to date. This means teams can start with a Postgres monolith for simplicity and fast time to market, and then effortlessly add Materialize as they scale and move to real-time microservices and analytics. Kafka will still continue to make sense as a central data streaming platform as more data needs to be shared amongst more systems, but it is no longer a hard prerequisite to build event-driven microservices. Over time, other first-class CDC integrations can be added (e.g. MySQL), further reducing friction for adoption (right now, you would have to use MySQL -\u0026gt; Debezium CDC -\u0026gt; Kafka -\u0026gt; Materialize). Materialize has first class support for the universally beloved Data Build Tool dbt, giving it the potential to quickly turn existing ETL data pipelines into superior real-time data pipelines (see docs example). While it\u0026rsquo;s not trivial to operate Materialize as a managed service, the company is hyper-specialized on building and operating only Materialize. Competitors are finding themselves operating multiple complex distributed technologies, requiring several different, specialized infrastructure teams. Materialize\u0026rsquo;s performance means it will effectively compete with other databases on cost, which bodes well in an economy with tightening IT budgets. Fun Threads Where I Learned a Lot About Materialize Hey #dataengineering Twitter, how should I think about @MaterializeInc vs real-time analytics databases like @ApachePinot or @druidio ?\n\u0026mdash; Chuck Larrieu Casias (@MrLarrieu) September 22, 2022 Building CQRS Views with @debezium, @apachekafka, @MaterializeInc, and @ApachePinot I tried to reimagine an online pizza order tracker architecture, which queries an incrementally updated read-optimized view.\nPart 1 - https://t.co/rZCbKFEyYS Part 2 - https://t.co/HnSHQrf40J pic.twitter.com/DcSnjiy6fI\n\u0026mdash; Dunith Dhanushka 🇱🇰🥑 (@dunithd) August 11, 2022 Thinking and learning out loud — data normalization vs denormalization. The big idea that’s sinking in for me is that normalization is better for writes, and denormalization is better for reads. Short thread:\n\u0026mdash; Chuck Larrieu Casias (@MrLarrieu) October 14, 2022 Questions I\u0026rsquo;m Curious to Research How does ingestion from Kafka work? Does it use librdkafka Rust client? Can it scale beyond input partitions like the Confluent Parallel Consumer? The demo on that page shows 500 records/s for the vanilla consumer -\u0026gt; 20,000 records/s for the parallel consumer. This is because the vanilla consumer processes records in a single thread while the parallel consumer processes records with multiple threads in parallel while allowing the user to choose ordering guarantee (slowest: partition based ordering, faster: key-based ordering, fastest: unordered). Does Materialize require co-partitioning of input topics like Kafka Streams and Flink so that a given key from input topics always lands in the same stream processing task? Given strong consistency, should folks avoid Materialize for use cases where lots of late-arriving data is expected (e.g. edge devices sending records under spotty network conditions)? What\u0026rsquo;s the story for selling and supporting on-prem Materialize deployments? I suspect having local, strongly consistent, real-time materialized views at the edge or on-prem would make for some interesting use cases! How does Materialize perform outside of steady state stream processing workloads, like Backfilling? Bootstrapping? Catch up? Bursty? See this talk from Immerok to see how Flink handles these things Materialize is great for reading the current state right now, but what about historical aggregations? Kafka Streams and ksqlDB have the concept of time windows and maintains these windows in memory for a retention period (default 1 day). If I want to look at the aggregation of a column over, say, 1 hour time windows for the last month, does Materialize have a way to specify and maintain incrementally updated views of those windows, and do they also have a retention time? Would we aggregate over time buckets like this docs example? Without window retention, how would the system not run out of memory as time progresses? Perhaps these sorts of historical use cases are where it pays to use Materialize in conjunction with Pinot/Druid, as in Dunith\u0026rsquo;s pizza example? Conclusion This is going to be fun! I can\u0026rsquo;t wait to help folks solve cool problems with Materialize! If you liked this post, then consider following me on Twitter and LinkedIn, and consider sharing with the social media buttons below.\n","permalink":"https://chuck-alt-delete.github.io/posts/first-day-at-materialize/","summary":"Every so often, I decide to turn my life upside down. A bit over a week ago, I made this announcement:\n📢Anouncement📢\nSurprising even myself, I have decided to move on from @confluentinc and this will be my last week. Confluent is an amazing company, and in particular, @jwfbean , David Shook, and Maygol Kananizadeh have been such amazing teammates. 1/3\n\u0026mdash; Chuck Larrieu Casias (@MrLarrieu) October 24, 2022 But why leave a great company (which, by the way, just beat expectations for the 6th consecutive quarter, despite the recession)?","title":"First Day at Materialize"},{"content":"I\u0026rsquo;m really excited to share that on Monday, December 6, I\u0026rsquo;ll be joining the technical marketing team at Confluent!\nGoodbye, Curriculum Development I\u0026rsquo;ve had an amazing time learning and teaching data in motion as a curriculum developer in Confluent\u0026rsquo;s Education organization. I put a lot of energy into building curriculum development almost from the ground up, and it\u0026rsquo;s amazing to see the seeds I\u0026rsquo;ve planted grow into formidable saplings.\nReflecting on my time in curriculum development, here are a couple of things I\u0026rsquo;m proud of:\nBuilt tooling and processes that dramatically increased developer productivity Created an automated virtual machine image pipeline with Ansible and Packer so that we could create any lab environment at will Supported and helped develop the Confluent Lab Portal, which allows customers to launch VMs in one click from the AMIs created by the image pipeline Became a subject matter expert in event streaming, security, and various Confluent products Overhauled several instructor led courses Created an awesome introductory course about Confluent Platform Created an awesome course for ksqlDB Created an awesome Role Based Access Control course This is probably where I learned the most. It\u0026rsquo;s about Confluent RBAC, but it also faithfully follows all security best practices. I had to dive deep into security of every component in the system. It felt like I was a security consultant for a startup. The lab environment for this course ended up being reused for half a dozen other security courses. Created a simple but super important course about Kafka listeners Created an awesome course about Confluent for Kubernetes (FREE!) Created an awesome course about Ansible Playbooks for Confluent Platform Pioneered the use of Gitpod at Confluent to create 1-click, hands-on experiences For example, run Confluent Platform Demo in Gitpod Learned a lot about the observability space and created a pretty sweet lab dedicated to Kafka client observability Check it out \u0026ndash; https://github.com/chuck-confluent/kafka-observability (and you can run it yourself in the browser with Gitpod!) I want to thank the entire Confluent Educaton team for their support. In particular, I give special thanks to my colleagues Gabriel Schenker, Russ Sayers, and Dave Shook, from whom I\u0026rsquo;ve learned so much, and to my manager Shira Gotshalk, who has been an amazing advocate for the team and set us in a great direction.\nI can\u0026rsquo;t share anything yet, but the Education team is working on some projects that will make reverberations throughout Confluent. It\u0026rsquo;s sad to leave before these projects come to light, but I\u0026rsquo;m happy to have been a part of them, and I will cheer when they release!\nWhile I enjoyed my work in curriculum development, it was was time to move closer to the challenges that exist out in the market. I want to convince folks to dramatically improve their data infrastructure by setting their data in motion with Confluent products. Once I get them in the door, my friends in Professional Services and Education will make sure they succeed.\nWhat is Technical Marketing? I never ever ever thought I would be in marketing. Images conjure in my mind of Mad Men smoking cigars, thinking about ways to convince people to buy things they don\u0026rsquo;t need, or of pop-up ads filling the screen when I\u0026rsquo;m away from my beloved AdBlock and PiHole.\nIt turns out that when you work at a company with really cool products that are actually transforming the way data is handled in the world, marketing is pretty fun!\nI\u0026rsquo;m new to this, but from what I understand, technical marketing is about convincing folks to buy your product not just by telling them how it will improve their lives but by actually showing them with deep, fully fleshed-out working models. To be convincing to a technical audience, we not only have to communicate effectively, but we also need our stuff to work and work well.\nTechnical Marketing at Confluent Confluent has an amazing Developer Experience (DevX) team that does something very similar to technical marketing. Our developer advocates also create demos, code examples, videos, blog posts, and presentations. The difference is in emphasis. Developer Experience is not primarily about selling products. It\u0026rsquo;s about fostering community and making it easier for developers to learn and participate, with the hope that happy developers will someday translate to happy customers.\nAs a Technical Marketing Manager, I am ALL business. Where DevX focuses on making it fun and easy for developers to build event streaming apps, the technical marketing team convinces executives and technical leaders to invest in next-generation data infrastructure. We do this by creating specific examples of how Confluent products solve business problems. Here are a couple of really high quality examples from the technical marketing team:\nBlog: Accelerate Your Cloud Data Warehouse Migration and Modernization with Confluent video demo Blog: Messaging Modernization with Confluent github repo Note By the way, if you\u0026rsquo;re like me, you might find the title \u0026ldquo;Technical Marketing Manager\u0026rdquo; to be confusing. I\u0026rsquo;m not a people manager. I manage technical marketing content. Promoting Confluent The underlying technology is so good that our biggest competition is actually people running open source Apache Kafka themselves. That seems cheap at first, but I can assure you it is in fact very difficult to own and operate critical infrastructure like Kafka at enterprise scale. You end up hiring teams of expensive platform engineers to work full time on operations that don\u0026rsquo;t directly add business value. At Confluent, we have the lion\u0026rsquo;s share of Kafka expertise. We can run Kafka for you better and cheaper than you can run yourself, and we can support you better than anyone else can.\nBeyond just being really good at operating Kafka clusters, Confluent has a bunch of products that are table stakes for enterprise use cases. I look forward to creating deep technical content to showcase these features and set data in motion around the world!\n","permalink":"https://chuck-alt-delete.github.io/posts/technical-marketing/","summary":"I\u0026rsquo;m really excited to share that on Monday, December 6, I\u0026rsquo;ll be joining the technical marketing team at Confluent!\nGoodbye, Curriculum Development I\u0026rsquo;ve had an amazing time learning and teaching data in motion as a curriculum developer in Confluent\u0026rsquo;s Education organization. I put a lot of energy into building curriculum development almost from the ground up, and it\u0026rsquo;s amazing to see the seeds I\u0026rsquo;ve planted grow into formidable saplings.\nReflecting on my time in curriculum development, here are a couple of things I\u0026rsquo;m proud of:","title":"Technical Marketing at Confluent"},{"content":" Introduction This lab is modified from one of my courses over at Confluent Education. The course is about using Confluent’s Ansible playbooks, but I had so much fun setting up the lab environment that I thought others would too. This lab environment is cool because you can simulate your own Virtual Private Cloud locally using Ansible’s Docker connection.\nThe key to making this work is to use Jeff Geerling’s docker-\u0026lt;distro\u0026gt;-ansible Docker images and mounting the /sys/fs/cgroup into the container. This allows the container to use systemd, making it behave like a VM you might have running in the cloud. Here’s one of Jeff’s ubuntu images, for example.\nClone my source repository and have fun with the lab!\nLearning Objectives This lab has two purposes:\nHave fun with Ansible’s Docker connection to sumulate a Virtual Private Cloud, locally\nPractice some Ansible fundamentals\nHere are the Ansible fundamental’s we’ll be looking at:\nInventories\nPlaybooks\nVariables\nAnsible Vault\nModules\nRoles\nLecture Ansible Architecture Ansible provides an automation engine to configure remote systems at scale. Ansible uses YAML (Yet Another Markup Language) files to provide a straightforward syntax rather than endless bash scripts. Ansible strives to make its configurations idempotent, which means that it can be run multiple times with the same effect as running once. This helps to save time on subsequent runs by skipping tasks that don’t need to be repeated.\nAnsible has a \u0026#34;push\u0026#34; architecture. There is a central machine called the Ansible control node (or controller) that has ssh access to a set of target hosts. There are no extra servers, daemons, or databases required.\nAnsible is written in Python, and comes with many built-in Python programs called modules. The Ansible control node establishes an ssh connection to the target hosts, loads the modules onto the hosts, runs them, and then removes them after it is finished.\nThere is a vibrant Ansible community that provides much more functionality through the Ansible Galaxy website, including custom modules, roles, and collections.\nActivity The purpose of this activity is to practice hands-on with a typical Ansible workflow, which includes:\nInstalling Ansible in a Python virtual environment\nInspecting a host inventory file with multiple groups\nUsing a module to execute code remotely on an Ansible target host\nInspecting playbooks\nDefining variables under the group_vars directory\nEncrypting variables with Ansible Vault\nPrerequisites Python 3\nDocker\nDocker Compose\nClone the source repository\nInstall Ansible Ansible is a tool written in Python. It is considered a best practice in Python to install dependencies in a \u0026#34;virtual environment\u0026#34; to avoid conflicts with the system’s Python packages. Here, we install Ansible in a Python virtual environment.\nCreate a virtual environment called testenv located in the root of this repo.\npython3 -m venv ./testenv The -m option specifies a module to run, which in this case is the venv module. Activate the virtual environment.\nsource ./testenv/bin/activate The prompt should now show (testenv) to indicate that you are using the virtual environment. We can now use python and pip instead of python3 and pip3, since the virtual environment was created with Python 3. You can verify this with python --version. Update pip, the Python package manager\npython -m pip install --upgrade pip Install Ansible with pip\npython -m pip install ansible-core Install some required \u0026#34;collections\u0026#34; with ansible-galaxy, Ansible’s own \u0026#34;package manager\u0026#34; of sorts.\nansible-galaxy install -r requirements.yml \u0026amp;\u0026amp; \\ ansible-galaxy collection list Collections are public Ansible repositories available on Ansible Galaxy. In this case, the collections have already been installed for you. Explore a Host Inventory This lab environment uses Docker Compose to simulate virtual machines running in a remote datacenter.\nInspect docker-compose.yml and create the hosts host1.test.confluent and host2.test.confluent.\ndocker-compose up -d The -d option means \u0026#34;detached\u0026#34;, which means that server logs won’t print to standard output. Inspect the host inventory file hosts.yml. What do you notice? What do you wonder? Write down a few thoughts and compare to the sample response.\nsample response Ansible uses an inventory file to describe the hosts it will configure. The creator of the inventory file can categorize different hosts into groups and label the groups whatever they want.\nThe only group Ansible requires is all, which refers to all hosts defined anywhere in the inventory. The variables (vars) defined under the all group apply to all hosts. This is where ansible_connection, and other global variables are defined. In this case, we use the docker connection to connect to docker hosts, but usually this connection will be ssh.\nIn Ansible, become refers to which user is used on the target host. Usually become is set to true and the user is some user with root privileges. This allows the Ansible control node to use elevated privileges to install software.\nThis inventory file has two user-defined groups of hosts:\natlanta — this group is called \u0026#34;atlanta\u0026#34;, perhaps to specify that hosts in this group are geographically located in Atlanta\nboston — again, this group is probably named to denote the geographical region of the hosts\nRun a Module on the Hosts Ansible uses modules to run programs on remote hosts. There are many modules built into Ansible, and more modules can be installed through Ansible Galaxy collections.\nRun the ping module against the hosts in the atlanta group.\nansible -m ping \\ -i hosts.yml \\ atlanta You should see output from the host that responds with \u0026#34;pong\u0026#34; Run the ping module against the hosts in the boston group.\nansible -m ping \\ -i hosts.yml \\ boston Run the ping module against all hosts.\nansible -m ping \\ -i hosts.yml \\ all Explore Playbooks In the Ansible world, a playbook is a YAML file that defines what will execute on target hosts.\nInspect the file playbook_atlanta.yml. What do you notice? What do you wonder? Write down a few thoughts and compare against the sample response.\nsample response This playbook defines a couple of tasks to be run againsts hosts in the atlanta group. A task specifies a human-readable name, a module, some specifications for the module, and perhaps some task-specific variables.\nThere is a variable called awesome_string that is set to the value of another variable, called vault_awesome_string using Jinja variable templating with double brackets — \u0026#34;{{ …​ }}\u0026#34;. The actual value of the vault_awesome_string variable will be explored in the next section.\nThe first task uses the built-in shell module to run a shell command and register the output to a new variable called response.\nThe second task uses the debug module to output some contents of the response variable.\nThe third task uses the built-in yum module to install the Apache httpd webserver package with the yum package manager.\nRun the playbook_atlanta.yml playbook against the host inventory\nansible-playbook \\ playbook_atlanta.yml \\ -i hosts.yml Notice the tasks only ran on hosts in the atlanta group, as specified in the playbook. Inspect the file playbook_all.yml. What do you notice? What do you wonder? Write down a few thoughts and compare against the sample response.\nsample response This playbook runs against all hosts instead of just the atlanta hosts\nThe playbook uses inventory_hostname and group_names variables, which are built-in Ansible variables that capture information about each host\nThis playbook imports the other playbook, so playbook_atlanta.yml will run against the hosts in the atlanta group as well\nRun the playbook_all.yml playbook against the host inventory and notice what happens.\nansible-playbook \\ playbook_all.yml \\ -i hosts.yml Set Group Variables with group_vars and Encrypt Secrets with Ansible Vault It is common practice to use a group_vars directory to define variables for different groups of hosts. Furthermore, it is important to encrypt variables with sensitive credentials so they aren’t compromised in source control.\nNotice that there is a directory called group_vars at the same level as the inventory file hosts.yml in `. Further notice that under `group_vars, there is another directory called atlanta. This is no accident. Ansible looks for directories under group_vars whose names correspond to host groups. Any variables defined in YAML files in group_vars/atlanta will be available for hosts in that group.\nInspect the file group_vars/atlanta/vault.yml. Notice that this is where the variable vault_awesome_string is defined for hosts in the atlanta group.\nIt is vital to encrypt sensitive credentials so they aren’t exposed in source control. Use Ansible Vault to encrypt vault.yml using the password confluent when prompted.\nansible-vault encrypt group_vars/atlanta/vault.yml You should now see the contents of vault.yml have been encrypted. If you want to view the unencrypted contents, you can run ansible-vault view group_vars/atlanta/vault.yml. Run playbook_atlanta.yml on the hosts again, using vault password confluent.\nansible-playbook --ask-vault-pass \\ playbook_atlanta.yml \\ -i hosts.yml Notice the tasks are able to access the encrypted variables. Note that it is not secure to use the debug module to view encrypted secrets in the task output. This was only done for demonstration purposes. If you want to suppress the output of certain tasks, you can set the built-in no_log variable to True on those tasks. (Optional) Create a Sample Role Playbooks can get crowded and hard to reason about. Ansible uses the concept of a role to package up related tasks to be shared and referenced in playbooks.\nCreate an Ansible role called sample-role.\nansible-galaxy role init sample-role Take note of the folder structure and inspect some of the files. This will be discussed more in the activity debrief.\nClean Up Destroy your hosts.\ndocker-compose down Activity Debrief What is an Ansible Role? tasks The tasks directory is the most important part of the role. The files in this directory define the tasks that will run on the hosts. When first learning what a role does, it is a good idea to start in this directory.\ntemplates Templates generate files that will end up on the hosts. Ansible uses the Jinja templating engine, which enables files to be created with conditional logic and variables. Here is an example of a Jinja template from Ansible Playbooks for Confluent Platform that generates Kafka broker server.properties files:\nserver.properties.j2 1 2 3 4 5 # Maintained by CP Ansible {# kafka_broker_final_properties defined in the confluent.variables role #} {% for key, value in kafka_broker_final_properties|dictsort%} {{key}}={{value}} {% endfor %} Line 1 is text that will appear literally in the server.properties file.\nLine 2 is a Jinja comment, so it won’t appear in the server.properties file.\nLine 3 shows a for loop that iterates through a dictionary and sorts the dictionary with Jinja’s built-in dictsort filter.\nLine 4 puts text in the file according to the values of those variables.\nLine 5 ends the for loop.\nThe end result is for several lines of text to appear in the file according the entries in the kafka_broker_final_properties dictionary.\nIt is important to note how Jinja uses braces {} to set an execution context and double braces {{}} to reference variables.\ndefaults The values given to variables in this defaults directory are default values that are used if you don’t override them elsewhere. These values have the lowest precendence and are easily overridden. Usually, you assign values to variables in the appropriate group_vars subdirectory alongside your host inventory file, and any variables you didn’t explicitly assign will be given their default values from this defaults directory.\nThere is a reference to variable precedence documentation in the References section.\nhandlers The handlers directory is home to special tasks called handlers. A handler is a task that triggers only when the state of something changes. A handler is notified of a change using the notify keyword.\nHere is an example of a task in tasks/main.yml that notifies a handler called restart kafka.\ntasks/main.yml - name: Write Service Overrides template: src: override.conf.j2 dest: \u0026#34;{{ kafka_broker.systemd_override }}\u0026#34; mode: 0640 owner: \u0026#34;{{kafka_broker_user}}\u0026#34; group: \u0026#34;{{kafka_broker_group}}\u0026#34; notify: restart kafka And here is the corresponding handler in the handler directory:\nhandlers/main.yml - name: restart kafka systemd: daemon_reload: true name: \u0026#34;{{kafka_broker_service_name}}\u0026#34; state: restarted In this example, the task creates a systemd service override file from a template. If the file doesn’t exist or changes, it notifies the handler named restart kafka. The restart kafka handler uses the systemd module to reload systemd and restart the Kafka service.\nmeta Files in the meta directory provide information about the role itself (metadata). This could include information about the author of the role, the namespace on Ansible Galaxy where you can find the role, role dependencies, and other metadata.\ntests This directory usually has a sample inventory file, e.g. test-hosts.yml, that points to localhost or hosts in a test environment. The inventory would be alongside a playbook, e.g. test.yml, that calls the role with import_role. Runing the playbook tests the role on the test hosts.\nIn practice, many role authors choose to use Ansible Molecule as a testing framework. Ansible Molecule uses Docker to create hosts and test different scenarios. We will see this in a later learning module.\nPlaybooks can get crowded and hard to reason about. Ansible uses the concept of a role to package up related tasks to be shared and referenced in playbooks with an import_role task. Roles are organized into all the parts shown in this slide.\nThis is just a brief overview. In later learning modules, you will look at roles in the CP Ansible project in more detail.\nDiscussion Questions What do the different parts of this Ansible command do?\nansible -m ping \\ -i hosts.yml \\ boston sample response This command executes the ping module on hosts in the boston group of the hosts.yml inventory file.\nWhat do the different parts of this Ansible command do?\nansible-playbook \\ playbook_atlanta.yml \\ -i hosts.yml sample response This command runs the playbook playbook_atlanta.yml file on hosts in the hosts.yml file.\nWhat is the group_vars directory and how does it work to define variables for different groups of hosts?\nsample response Ansible looks for directories under group_vars whose names correspond to host groups defined in a host inventory file.\nExample: Any variables defined in YAML files in the group_vars/all/ directory will be available for all hosts.\nExample: If there is a host group called atlanta, then the variables defined in YAML files in the group_vars/atlanta/ directory will be available for hosts in the atlanta group.\nWhat is Ansible Vault and why is it important?\nsample response Ansible Vault refers to the ansible-vault command line utility. Ansible Vault allows you to encrypt files that contain sensitive credentails. This is important because Ansible projects are often source controlled in shared code repositories, and the set of people who have read access to the repository may be different from the set of people who should have access to the sensitive credential.\nThe password for a file encrypted with Ansible Vault should be stored securely in a password manager or secure credential storage service like Hashicorp Vault. A common workflow is to authorize the Ansible control node to access relevant Ansible Vault passwords from a credential storage service and then retrieve the passwords at runtime using a password client script.\nSummary In this module, you practiced hands-on with a typical Ansible workflow:\nInstalling Ansible in a Python virtual environment\nInspecting a host inventory file with multiple groups\nUsing a module to execute code remotely on an Ansible target host\nInspecting playbooks\nDefining variables under the group_vars directory\nEncrypting variables with Ansible Vault\nWith this workflow, you reviewed fundamental Ansible concepts:\nInventories\nPlaybooks\nVariables\nAnsible Vault\nModules\nRoles\nReferences Ansible documentation\nYAML Syntax Reference\nIndex of all Ansible modules\nAnsible Vault documentation\nAnsible Variable Precedence\nAnsible Galaxy\nComprehensive Ansible 101 video series by Jeff Geerling\nAnsible for DevOps Examples by Jeff Geerling\nList of built-in Jinja filters\n","permalink":"https://chuck-alt-delete.github.io/posts/ansible-docker/","summary":"Introduction This lab is modified from one of my courses over at Confluent Education. The course is about using Confluent’s Ansible playbooks, but I had so much fun setting up the lab environment that I thought others would too. This lab environment is cool because you can simulate your own Virtual Private Cloud locally using Ansible’s Docker connection.\nThe key to making this work is to use Jeff Geerling’s docker-\u0026lt;distro\u0026gt;-ansible Docker images and mounting the /sys/fs/cgroup into the container.","title":"Fun with Ansible's Docker Connection"},{"content":" Foreword From 2021 A while ago, I listened to an amazing podcast episode about DynamoDB decided to do one of AWS’s DynamoDB hands-on tutorials and create a little presentation about what I learned to share at a meetup.\nEnjoy!\nObjectives Get to know each other\nCompare and contrast DynamoDB data modeling to relational data modeling\nDescribe the importance of partitioning in DynamoDB\nNon-Work Facts About Me! Chuck\nMy first job was at a doggy daycare\nI randomly know a lot about aphids\nI graduated with high honors in pure math and produced a thesis as an undergraduate\nI love building PCs to play big beautiful video games\nI am a podcast nerd:\nFreakonomics, Software Engineering Daily, Intelligence Squared, Not Another DnD Podcast, Radiolab, and more\nMy wife Caroline, my son Rafael, and my son Arlo are my best friends and make me laugh every day\nNotice and Wonder What do you notice?\nWhat do you wonder?\nNotice and Wonder Debrief NoSQL vs. Relational Data Modeling — Online Gaming Think of an online multiplayer game.\nWhat are some \u0026#34;entities\u0026#34; involved?\nWhat are some access patterns?\nThe Relational Way Suppose we want to query for all the usernames of users in a particular game.\nStar Schema Pros Cons Table for \u0026#34;user\u0026#34;\nTable for \u0026#34;game\u0026#34;\nFact table that relates \u0026#34;user_ID\u0026#34; with \u0026#34;game_ID\u0026#34;\nJoin \u0026#34;user\u0026#34; and \u0026#34;game\u0026#34; tables and filter for records with the given \u0026#34;game_ID\u0026#34;\nGreat for ad-hoc queries\nGreat for analytics, like aggregations\nJoins don’t scale well\nThe DynamoDB Way This is all one big, hyper-scalable table! It looks like two entity tables and a join table, but really it’s one big table where the entities and join are created by clever design of the primary and sort keys, which we’ll see in the next slide.\nNote One table with O(1) lookup by primary key, or O(log(n)) if including a sort key. No joins! You plan the access pattern into your data model and include everything in one table. Design the Primary Key Table 1. Composite Primary Key Entity Partition Key (a.k.a. HASH) Sort Key (a.k.a. RANGE) User\nUSER#\u0026lt;USERNAME\u0026gt;\n#PLACEHOLDER#\u0026lt;USERNAME\u0026gt;\nGame\nGAME#\u0026lt;GAME_ID\u0026gt;\n#PLACEHOLDER#\u0026lt;GAME_ID\u0026gt;\nUserGameMapping\nGAME#\u0026lt;GAME_ID\u0026gt;\nUSER#\u0026lt;USERNAME\u0026gt;\nCode Example Query multiple entities in one request to one table!\nTake a moment to think about what this query does:\ngame_id = input(\u0026#34;what game ID do you want to look up?\u0026#34;) resp = dynamodb.query( TableName=\u0026#39;battle-royale\u0026#39;, KeyConditionExpression=\u0026#34;PK = :pk AND SK BETWEEN :placeholder AND :users\u0026#34;, ExpressionAttributeValues={ \u0026#34;:pk\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;GAME#{}\u0026#34;.format(game_id) }, \u0026#34;:placeholder\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;#PLACEHOLDER#{}\u0026#34;.format(game_id) }, \u0026#34;:users\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;USER$\u0026#34; }, }, ScanIndexForward=True ) sample response This query takes a given game ID and looks up all the users in that game. Let’s say the game ID is abc123. The PK part of the KeyConditionExpression stands for \u0026#34;Primary Key\u0026#34;, which is the key used to determine which DynamoDB partition holds the data.\nNow that we have found the partition where game ID abc123 lives, the next step is to use the sort key, SK, to grab all the users. We look up records in order from :placeholder to :users, which are further specified in the ExpressionAttributeValues parameter. The :placeholder expression corresponds to the string #PLACEHOLDER#abc123 (type denoted by S). The expression :users corresponds to the string USER$.\nThe # symbol is 35 in ascii, and $ is 36, so USER$ is greater than all USER#\u0026lt;USERNAME\u0026gt; entries. That means this query will return the game followed by all the users in that game in ascending order by username. For example:\n{\u0026#34;GAME#abc123\u0026#34; : \u0026#34;#PLACEHOLDER#abc123\u0026#34;} {\u0026#34;GAME#abc123\u0026#34;: \u0026#34;USER#myuser1\u0026#34;} {\u0026#34;GAME#abc123\u0026#34;: \u0026#34;USER#myuser2\u0026#34;} {\u0026#34;GAME#abc123\u0026#34;: \u0026#34;USER#myuser3\u0026#34;} ... Next Step: Secondary Indexes Secondary indexes are very useful in DynamoDB. They allow you to query data by attributes other than the primary key. This opens up more access patterns. The benefits of secondary indexes come at the cost ($ literally $) of more reads and writes as DynamoDB automatically updates indexes. It is easy to use secondary indexes poorly, so I encourage you to dig into the tutorial in the reference to learn more about best practices with secondary indexes. Namely:\nSparse secondary index\nCreate an index that filters down to a specific subset that won’t grow indefinitely\nexample: create index to find all open games on a specific map\nInverted index\nSwitch the roles of primary key and sort key\nexample: find all games a user has played (opposite of what we did earlier when we found all users in a specific game)\n3, 2, 1 Reflection What are 3 things you learned?\nWhat are 2 things you found interesting?\nWhat’s 1 question you still have?\nObjectives Get to know each other\nDescribe the importance of partitioning in DynamoDB\nCompare and contrast DynamoDB data modeling to relational data modeling\nReferences DynamoDB Hands-on Tutorials\nSoftware Engineering Daily Podcast: DynamoDB with Alex Debrie\nDynamoDB Python Reference (boto3 library)\nAWS Docs: Improving Data Access with Secondary Indexes\n","permalink":"https://chuck-alt-delete.github.io/posts/dynamo-presentation/","summary":"Foreword From 2021 A while ago, I listened to an amazing podcast episode about DynamoDB decided to do one of AWS’s DynamoDB hands-on tutorials and create a little presentation about what I learned to share at a meetup.\nEnjoy!\nObjectives Get to know each other\nCompare and contrast DynamoDB data modeling to relational data modeling\nDescribe the importance of partitioning in DynamoDB\nNon-Work Facts About Me! Chuck\nMy first job was at a doggy daycare","title":"Learning DynamoDB"},{"content":"Foreword from 2021 I wrote this post for Insight Data Science in 2019. I\u0026rsquo;ve updated it a tiny bit, but it\u0026rsquo;s mostly the same. It was published on Dzone and the Insight Data Science blog:\nhttps://dzone.com/articles/how-to-prepare-for-your-devops-interview (40k views!) https://blog.insightdatascience.com/devops-interview-prep-guide-8cd3dc60e587 Introduction Motivation is a great start, but you\u0026rsquo;ll need the discipline to prepare effectively. I\u0026rsquo;ve written this guide to help you develop a disciplined interview preparation routine. Interviewing can really suck - it\u0026rsquo;s random and can make you feel bad about yourself. There\u0026rsquo;s so much you can\u0026rsquo;t control in the process, but you can control how you prepare. There is a lot here to chew on, so don\u0026rsquo;t expect to be able to learn everything all at once. Maybe one day you\u0026rsquo;ll choose one topic to focus on. Maybe you\u0026rsquo;ll focus on another topic for a week straight. In general, though, you should pick two or three activities to do habitually each day, rotating through different topics so your mind stays fresh and engaged. These categories are listed in order of importance:\nLinux Fundamentals Data Structures and Algorithms System Design Parsing DevOps Tools It\u0026rsquo;s good to develop a wide, shallow base of knowledge first, so load balance across topics in a round-robin fashion at first. That will widen your conceptual framework of the DevOps space so that you can more easily put new information into context. The activities within each category are ranked more or less in order of importance as well. Certain activities lend themselves to daily practice, like CodeSignal problems or OverTheWire challenges. Others lend themselves to deep study.\nIf you want to go fast, go alone. But if you want to go far, go together. Create a study group with your peers. Hold each other accountable. Do weekly or daily check-ins and info-sharing sessions. During those time that you feel unmotivated, remember that doing something is much better than doing nothing. During those times when you are too motivated, remember that sleep and exercise are every bit as important to your success as anything else. These activities are the stepping stones to the next phase of your life, but remember to allow yourself to become fascinated with the shape of each stone.\nLinux Fundamentals As a DevOps Engineer, you\u0026rsquo;re expected to know Linux as well as the back of your hand. Any questions about Linux is fair game, from the kernel to networking to command line commands.\nActivities: Work through The Linux Command Line book Work through the OverTheWire wargames. These are fun scavenger hunts through Linux systems that are well-designed to teach you the fundamental concepts. At least get through the Bandit levels. Don\u0026rsquo;t be afraid to Google solutions if you get stuck. I highly suggest you document your progress in GitHub as you go. Learn vim by simply using the vimtutor command. It\u0026rsquo;s just the best editor ever and don\u0026rsquo;t let anyone convince you otherwise. Consider paying for acloudguru courses to get certifications in Linux fundamentals (Linux+, RedHat, etc.). It\u0026rsquo;s true that a pound of certifications is less valuable than an ounce of experience, but when you are low on experience, certifications can help fill in gaps in your knowledge and prove that you have a basic understanding. References TLDR.sh: The best reference manual for Linux commands. I highly suggest you download the tl;dr mobile app for quick reference. This has helped me on phone interviews where I didn\u0026rsquo;t know a command off the top of my head. devhints.io is a great source for all manner of cheat sheets. It doesn\u0026rsquo;t look flashy, but the Advanced Bash Scripting Guide is a goldmine. Data Structures and Algorithms Remember that DevOps is Development + Operations, so you\u0026rsquo;re expected to know how to code. Choose a programming language and practice, practice, practice. It\u0026rsquo;s not just about finding an optimal solution - you must clearly communicate the parameters of the problem with your interviewer, convey your thought process as you go along, and calculate the time and space complexity of different algorithms.\nActivities Khan Academy has a really nice data structures and algorithms primer. It\u0026rsquo;s in Javascript, but don\u0026rsquo;t hold that against it. This curriculum was made in partnership with Tom Cormen, the author of the definitive book on data structures and algorithms that\u0026rsquo;s definitely worth owning a copy of. LeetCode has a large repository of problems that are close to real interview questions. In fact, many companies will directly ask you LeetCode questions! Do at least 10 easy and medium questions of each type of algorithms. You can start to feel prepared when you\u0026rsquo;ve done 100 problems and can actually solve each without looking anything up. Work through the interview prep material daily on CodeSignal. I suggest Python, but you can work in any language. Copy and paste your code into files and version control it with GitHub. Each day, look through the code you did yesterday and document it in detail. Writing code is easier than reading your old code and trying to remember what\u0026rsquo;s going on. Revisiting your old code after a day or two will force you to synthesize the concepts you are learning. Work through CodeSignal\u0026rsquo;s Python Arcade. It is a nice tour of all the important standard Python libraries. Some of the exercises really miss the mark but, overall, this is a nice playlist. Research Dynamic Programming, Directed Acyclic Graphs, Topological Sort. Everyone needs a little SQL. No need to spend large amounts of time on this, but you should sign up for a free account on Mode Analytics and make use of their great interactive SQL tutorials that use real public databases. As you get more comfortable with your coding skills, use Pramp to practice live technical interviews. This is an important component. Nothing quite prepares you for technical phone screens like practicing technical phone screens. Remember that the only knowledge that counts is what you can demonstrate in front of the interviewer, either on the whiteboard or live coding. If you tend to freeze up in high-pressure situations, you\u0026rsquo;re far from the only one - be sure to do lots of practice with peers. References LearnXinYminutes is a great, example-driven cheat sheet for Python Hitchhiker\u0026#39;s Guide to Python: The best Python documentation. My scripts GitHub repo has lots of examples of thoroughly documented code, although it\u0026rsquo;s really unorganized! System Design The system design interview tests your big-picture thinking, ability to architect complex infrastructure, and communication skills. Be sure to practice a top-down design approach with clear infrastructure diagrams and DevOps considerations.\nActivities Chip away at the System Design Primer. One fun thing to do is to pick a topic and ask a friend to pick another topic, then spend 2 hours researching your topic before coming back together for an hour or two to share. Take turns playing the roles of interviewer and interviewee. Read the Google SRE Book. This is a must-read book for modern DevOps. Parsing Sysadmins frequently analyze system outputs and logs to obtain system insights and debug issues. Command-line parsing with tools like grep, awk, and sed is an important skill for the DevOps arsenal.\nActivities Learn regular expressions (RegEx) with hands-on exercises at RegExOne Work through the exercises in the Insight parsing workshop materials. Focus on awk. The chapter materials are good, but the exercises are the most important part to do. References LearnXinYminutes: Another great example-driven reference - this time for awk. Regexr: The best regular expression reference. DevOps Tools Docker, Terraform, and Kubernetes are essential DevOps tools, and you can\u0026rsquo;t go wrong learning them thoroughly. You should also know at least one CI/CD tool, such as Jenkins. It\u0026rsquo;s highly recommended that you learn these tools on at least one of the big three clouds: AWS, GCP, or Azure.\nActivities Containers You need to learn Kubernetes. In order to learn Kubernetes, you need to learn about containerization and Docker.\nWork through the Docker for Beginners lab Work through the Play With Docker Classroom Kubernetes (concept: container orchestration) Read through Kubernetes concepts Work through the Kubernetes Basics tutorial Check out Jeff Geerling\u0026rsquo;s awesome Kubernetes 101 video series For a deeper dive, work through the Katacoda course Continuous Integration / Continuous Delivery Create some automated testing and/or deployment on one of your GitHub repos with GitHub Actions Work through an introductory Jenkins tutorial or two to learn more about CI/CD (concept: deployment pipeline as code). I honestly don\u0026rsquo;t know that much about Jenkins, so I don\u0026rsquo;t know better resources for this. Infrastructure and Configuration as Code Use Vagrant to declaratively define a VM, and then use VirtualBox or Packer to export that VM as an OVA file or other machine image (concepts: immutable infrastructure, configuration as code). Terraform (concept: infrastructure as code) Read Terragrunt\u0026rsquo;s Comprehensive Guide to Terraform Work through the Terraform Tutorial Ansible (concept: configuration as code) Check out Jeff Geerling\u0026rsquo;s awesome Ansible 101 video series For some hands-on labs, I think Linux Academy might be the best bet (not free) Observability, Secrets Management, Service Mesh, Big Data Do the Honeycomb Quickstart to give a flavor of monitoring/tracing tools (concept: full stack tracing) Work through the Vault tutorial (concept: enterprise-scale secrets management) Work through the Consul tutorial (concept: service mesh). Play with Linkerd (concept: service mesh) Apache Kafka (concept: fault tolerant, high throughput event streaming) Shameless plug for Apache Kafka - go to developer.confluent.io to get started, and if you like that, ask your employer to pay for the education subscription (that\u0026rsquo;s what I write!) References Ansible: The User Guide Want to do something specific in Kubernetes? Check the Tasks section of the docs. There are many task-specific guides. Docker\u0026rsquo;s Samples page has good examples of fully built applications as well as some good hands-on tutorials. I like the Django\u0026#43;Postgres sample. Awesome Docker Compose is a great repository of docker-compose examples Other High Value Items Activities Listen regularly to the Software Engineering Daily podcast. This is by far the best way to keep a finger on the pulse of the DevOps world. I suggest getting the app so that you can easily search through old episodes for in-depth talks about the technologies and concepts you are studying. In the DevOps Tools section, I gave keywords for the concepts behind each tool. You can use these keywords to search SEdaily for relevant podcasts. Read about Kanban boards and use them to enhance your focus and productivity. GitHub has a simple kanban board you can use to keep yourself productive. I also personally like bullet journaling and the Pomodoro Method. Dig deep into the technical details of some of your favorite tools. Start with the documentation. Pair up and share your takeaways from the technical deep-dives. Consider paying for Linux Academy or acloudguru courses to get AWS, GCP, or Azure certifications. References Digital Ocean has great tutorials. When I Google, I tend to try\n\u0026lt;research topic\u0026gt; site:digitalocean.com first. This is especially helpful when you are first learning to install something manually.\n","permalink":"https://chuck-alt-delete.github.io/posts/devops-interview/","summary":"Foreword from 2021 I wrote this post for Insight Data Science in 2019. I\u0026rsquo;ve updated it a tiny bit, but it\u0026rsquo;s mostly the same. It was published on Dzone and the Insight Data Science blog:\nhttps://dzone.com/articles/how-to-prepare-for-your-devops-interview (40k views!) https://blog.insightdatascience.com/devops-interview-prep-guide-8cd3dc60e587 Introduction Motivation is a great start, but you\u0026rsquo;ll need the discipline to prepare effectively. I\u0026rsquo;ve written this guide to help you develop a disciplined interview preparation routine. Interviewing can really suck - it\u0026rsquo;s random and can make you feel bad about yourself.","title":"How to prepare for your DevOps Interview"},{"content":"This is the first post for Chuck+Alt+Delete! Welcome!\nThis site has two purposes:\n\u0026ldquo;learn in public\u0026rdquo; through blog posts host my professional portfolio Learn in Public I was inspired by the #LearnInPublic hashtag on twitter. Learning in public makes one feel vulnerable, but the upsides in terms of community and learning can be tremendous. Even this post might evolve over time. This is a place for rough draft thinking.\nPortfolio Hop over to my portfolio!\n","permalink":"https://chuck-alt-delete.github.io/posts/hello-world/","summary":"This is the first post for Chuck+Alt+Delete! Welcome!\nThis site has two purposes:\n\u0026ldquo;learn in public\u0026rdquo; through blog posts host my professional portfolio Learn in Public I was inspired by the #LearnInPublic hashtag on twitter. Learning in public makes one feel vulnerable, but the upsides in terms of community and learning can be tremendous. Even this post might evolve over time. This is a place for rough draft thinking.\nPortfolio Hop over to my portfolio!","title":"Hello, World!"},{"content":"Most of my work these last few years has lived in private repositories, so I\u0026rsquo;ve created this site to do more \u0026ldquo;learning in public.\u0026rdquo; I will keep this page updated with exemplars of my work.\nTechnical Marketing Assets As a Technical Marketing Manager at Confluent, I create and review blogs, whitepapers, demos, and videos. Here are a couple of assets to highlight:\nSIEM Optimization\nI heavily refactored a SIEM Optimization demo created by our enterprise sales engineers I added self-paced learning modules and executive brief documentation I added 0-setup, 1-click deployment with Gitpod (click here to run demo in your browser) Demo Repo Confluent Platform Demo\nI took ownership of our flagship showcase demo and am sole maintainer (as of writing) I refactored the demo to use Confluent Cluster Linking instead of Confluent Replicator and added Schema Linking GitHub Repo: https://github.com/confluentinc/cp-demo 10x Storage (10x Campaign)\nVideo conversation Demo repo: Training Machine Learning Models using Confluent Infinite Storage Trust Confluent Campaign\nDemo repo Video Courses As a Senior Curriculum Developer at Confluent, I have written many courses related distributed systems, databases, and stream processing. I\u0026rsquo;ll highlight two courses here. Sign up for the free self-paced courses and then check out:\nAutomate Deployment with Confluent for Kubernetes Whole course is free, including fully managed hands-on labs Labs are available on GitHub at https://github.com/confluentinc/confluent-kubernetes-examples In particular, I contributed heavily to: Configure external access with host-based routing Production grade security See my merged pull requests for details Create an Event Streaming App with ksqlDB Whole course is free, including fully managed hands-on labs GitHub Profiles https://github.com/chuck-alt-delete https://github.com/chuck-confluent Coding Puzzles While I enjoy solving coding puzzles for fun, I have significant anxiety when doing these coding challenges in interview conditions under time pressure, and as a dad with young kids and a full time job, I can\u0026rsquo;t grind these out in my free time anymore like I used to in order to prepare for new interviews. I may therefore opt out of such interview questions and direct interviewers to this section of my portfolio instead.\nHere are some links to dozens of data structure and algorithm style coding puzzles that I\u0026rsquo;ve done. I mostly like to do them in Python, but I\u0026rsquo;m conversant in Java, Go, and Javascript as well. Ask me to explain any of the solutions and I\u0026rsquo;ll happily get into recursion, depth first search, breadth first search, topological sort, quicksort, mergesort, dynamic programming, regular expressions, hashmaps, arrays, trees, generators, minheaps, queues, stacks, functional programming, object oriented programming, whatever other nerdy things you\u0026rsquo;d like to talk about.\nhttps://github.com/chuckinator0/Projects/tree/master/scripts https://github.com/chuck-alt-delete/exercism/tree/main/python ","permalink":"https://chuck-alt-delete.github.io/posts/portfolio/","summary":"Most of my work these last few years has lived in private repositories, so I\u0026rsquo;ve created this site to do more \u0026ldquo;learning in public.\u0026rdquo; I will keep this page updated with exemplars of my work.\nTechnical Marketing Assets As a Technical Marketing Manager at Confluent, I create and review blogs, whitepapers, demos, and videos. Here are a couple of assets to highlight:\nSIEM Optimization\nI heavily refactored a SIEM Optimization demo created by our enterprise sales engineers I added self-paced learning modules and executive brief documentation I added 0-setup, 1-click deployment with Gitpod (click here to run demo in your browser) Demo Repo Confluent Platform Demo","title":"Portfolio"}]